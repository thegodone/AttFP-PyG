{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match same input of features\n",
    "from AttFPfeaturing import datagenerator, getdataloader\n",
    "#from Datas import dataloader\n",
    "import pandas as pd\n",
    "target_list = ['Result0']\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('esol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128/1128 [00:06<00:00, 181.58it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datagenerator(df, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = getdataloader(data, batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[791], edge_attr=[1618, 10], edge_index=[2, 1618], x=[791, 40], y=[64, 1])\n",
      "torch.Size([791, 40])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(data.x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.nn import Linear, BatchNorm1d, Dropout\n",
    "from torch.nn import Parameter as Param\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_add_pool, EdgePooling\n",
    "from torch_sparse import matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.typing import PairTensor, Adj, OptTensor, Size\n",
    "from torch_scatter import scatter_add\n",
    "import pickle\n",
    "\n",
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "\n",
    "class GatConvAtom2(MessagePassing):\n",
    "    \"\"\"\n",
    "     degenerate AtomConv => only do upscaling projection & leaky_relu\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_in_channels: int,  fingerprint_dim: int, dropout: float, bias: bool = True, debug: bool = False, **kwargs):\n",
    "        super(GatConvAtom2, self).__init__()\n",
    "\n",
    "        self.atom_in_channels = atom_in_channels\n",
    "        self.fingerprint_dim = fingerprint_dim\n",
    "        self.atom_fc = Linear(atom_in_channels, fingerprint_dim, bias=bias)\n",
    "        self.debug = debug\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: Union[Tensor,PairTensor], edge_index: Adj,\n",
    "                size: Size = None) -> Tensor:\n",
    "        \n",
    "        out = self.propagate(edge_index, x = x,  size=size)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, edge_index, size) -> Tensor:\n",
    "\n",
    "        x_i = F.leaky_relu(self.atom_fc(x_i)) # code 3 \n",
    "\n",
    "        return x_i   \n",
    "\n",
    "\n",
    "\n",
    "class GatConvAtom(MessagePassing):\n",
    "    \"\"\"\n",
    "    This function does only the atom embedding, not the molecule embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_in_channels: int, bond_in_channels: int, fingerprint_dim: int, dropout: float, bias: bool = True, debug: bool = False, step = 0, **kwargs):\n",
    "        super(GatConvAtom, self).__init__()\n",
    "\n",
    "        self.atom_in_channels = atom_in_channels\n",
    "        self.bond_in_channels = bond_in_channels\n",
    "        self.fingerprint_dim = fingerprint_dim\n",
    "        self.step = step\n",
    "\n",
    "        if  self.step == 0 : \n",
    "            self.atom_fc = Linear(atom_in_channels, fingerprint_dim, bias=bias)\n",
    "            self.neighbor_fc = Linear(atom_in_channels + bond_in_channels, fingerprint_dim, bias=bias)\n",
    "        self.align = Linear(2*fingerprint_dim, 1, bias=bias)\n",
    "        self.attend = Linear(fingerprint_dim, fingerprint_dim, bias=bias)\n",
    "        self.debug = debug\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.rnn = torch.nn.GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, x: Union[Tensor,PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \n",
    "        out = self.propagate(edge_index, x = x, edge_attr=edge_attr, size=size)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index: Adj, edge_attr: OptTensor, size) -> Tensor:\n",
    "\n",
    "        if self.debug:\n",
    "            print('a x_j:',x_j.shape,'x_i:',x_i.shape,'edge_attr:',edge_attr.shape)\n",
    "        if  self.step == 0 :\n",
    "\n",
    "            x_i = F.leaky_relu(self.atom_fc(x_i)) # code 3 \n",
    "\n",
    "            # neighbor_feature => neighbor_fc\n",
    "            x_j = torch.cat([x_j, edge_attr], dim=-1) # code 8\n",
    "            if self.debug:\n",
    "                print('b neighbor_feature i = 0', x_j.shape)\n",
    "            \n",
    "            x_j = F.leaky_relu(self.neighbor_fc(x_j)) # code 9\n",
    "            if self.debug:\n",
    "                print('c neighbor_feature i = 0', x_j.shape)\n",
    "            \n",
    "        # align score\n",
    "        evu = F.leaky_relu(self.align(torch.cat([x_i, x_j], dim=-1))) # code 10\n",
    "        if self.debug:\n",
    "            print('d align_score:', evu.shape)\n",
    "        \n",
    "        # We need to find a way to do edge pooling per atom \n",
    "        avu = EdgePooling.compute_edge_score_softmax(evu, edge_index, edge_index.max().item() + 1) # code 11\n",
    "        \n",
    "        #avu = softmax(evu, edge_index, None, edge_index.max().item() + 1)\n",
    "        if self.debug:\n",
    "            print('e attention_weight:', avu.shape)\n",
    "\n",
    "        c_i = F.elu(torch.mul(avu, self.attend(self.dropout(x_i)))) # code 12\n",
    "\n",
    "        if self.debug:\n",
    "            print('f context',c_i.shape)\n",
    "            \n",
    "        x_i = self.rnn(c_i, x_i)\n",
    "        if self.debug:\n",
    "            print('g gru',c_i.shape)            \n",
    "\n",
    "        return x_i   \n",
    "\n",
    "class GatConvMol(MessagePassing):\n",
    "    \"\"\"\n",
    "    This function does the molecule embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, fingerprint_dim: int, dropout: int, debug: bool = False, step = 0):\n",
    "        super(GatConvMol, self).__init__()\n",
    "        # need to find the correct dimensions \n",
    "        self.step = step\n",
    "        self.mol_align = Linear(2*fingerprint_dim,1)\n",
    "        self.mol_attend = Linear(fingerprint_dim,fingerprint_dim)\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.debug = debug\n",
    "        self.rnn = torch.nn.GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "\n",
    "    def forward(self, x: Union[Tensor,PairTensor], edge_index: Adj, size: Size = None) -> Tensor:\n",
    "        \n",
    "        out = self.propagate(edge_index, x = x, size=size)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index: Adj, size) -> Tensor:\n",
    "        if self.step == 0:\n",
    "            h_s =  torch.sum(x_i, dim=-1)\n",
    "            if self.debug:\n",
    "                print('pre-h_s:',h_s.shape,',x_i:', x_i.shape)            \n",
    "                \n",
    "            h_s =  h_s.unsqueeze(1).repeat(1, x_i.size(1)) # code 2\n",
    "            if self.debug:\n",
    "                print('1 mol_feature expanded',h_s.shape)\n",
    "\n",
    "        else:\n",
    "            h_s = x_i\n",
    "        \n",
    "        if self.debug:\n",
    "            print('2 activated_features', x_i.shape)\n",
    "             \n",
    "        esv = F.leaky_relu(self.mol_align(torch.cat([h_s, x_i], dim=-1))) # code 5\n",
    "        if self.debug:\n",
    "            print('3 mol_align_score:',esv.shape)\n",
    "        # this is a sotfmax per molecule  \n",
    "        asv = F.softmax(esv, dim=-1) # code 6\n",
    "    \n",
    "        if self.debug:\n",
    "            print('4 mol_align_score:',asv.shape)\n",
    "        \n",
    "        # this is not correct it should be more hs and not x_i there based on the paper supplementary table 3!\n",
    "        cs_i = F.elu(torch.mul(asv, self.mol_attend(self.dropout(h_s)))) # code 7 \n",
    "        if self.debug:\n",
    "            print('5 mol_context' ,cs_i.shape)\n",
    "            \n",
    "        x_i = self.rnn(cs_i, h_s) # code 8\n",
    "        \n",
    "        return x_i\n",
    "\n",
    "\n",
    "class AtomEmbedding(torch.nn.Module):\n",
    "    def __init__(self, atom_dim,  edge_dim, fp_dim, R=2, dropout = 0.2, debug=False):\n",
    "        super(AtomEmbedding, self).__init__()\n",
    "        self.R = R\n",
    "        self.debug = debug\n",
    "        self.conv = torch.nn.ModuleList([GatConvAtom(atom_in_channels=atom_dim, bond_in_channels= edge_dim, fingerprint_dim=fp_dim, dropout = dropout, debug=debug, step = i) for i in range(self.R)])  # GraphMultiHeadAttention\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for i in range(self.R):\n",
    "            if self.debug:\n",
    "                print(x.shape)\n",
    "            \n",
    "            x = self.conv[i](x, edge_index, edge_attr) # code 1-12\n",
    "            if self.debug:\n",
    "                print(x.shape)    \n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class AtomEmbedding2(torch.nn.Module):\n",
    "    def __init__(self, atom_dim, fp_dim, R=1, dropout = 0.2, debug=False):\n",
    "        super(AtomEmbedding2, self).__init__()\n",
    "        self.R = R\n",
    "        self.debug = debug\n",
    "        self.conv = torch.nn.ModuleList([GatConvAtom2(atom_in_channels=atom_dim, fingerprint_dim=fp_dim, dropout = dropout, debug=debug) for i in range(self.R)])  # GraphMultiHeadAttention\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.R):\n",
    "            if self.debug:\n",
    "                print(x.shape)\n",
    "            \n",
    "            x = self.conv[i](x, edge_index) # code 1-12\n",
    "            if self.debug:\n",
    "                print(x.shape)    \n",
    "        return x\n",
    "\n",
    "\n",
    "class MoleculeEmbedding(torch.nn.Module):\n",
    "    def __init__(self, fp_dim, dropout, debug, T=2):\n",
    "        super(MoleculeEmbedding, self).__init__()\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.conv =torch.nn.ModuleList([GatConvMol(fp_dim, dropout, debug, step = i) for i in range(self.T)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.T):\n",
    "            x = self.conv[i](x, edge_index) # code 1-7\n",
    "        return x\n",
    "\n",
    "class AttentiveFPdebug(torch.nn.Module):\n",
    "    def __init__(self, atom_in_dim, edge_in_dim, fingerprint_dim=200, R=1, T=1, dropout=0.2,  debug = True, outdim=1):\n",
    "        super(AttentiveFPdebug, self).__init__()\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.dropout = dropout\n",
    "        # call the atom embedding Phase\n",
    "        self.convsAtom = AtomEmbedding(atom_in_dim, edge_in_dim, fingerprint_dim, R, debug) \n",
    "        self.convsMol = MoleculeEmbedding(fingerprint_dim, dropout, debug, T )\n",
    "\n",
    "        # fast down project could be much more sofisticated! (ie  Feed Forward Network with multiple layers )\n",
    "        self.out = Linear(fingerprint_dim, outdim) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        x = F.dropout(self.convsAtom(x, edge_index, edge_attr), p=self.dropout, training=self.training) # atom Embedding       \n",
    "        x = F.dropout(self.convsMol(x, edge_index), p=self.dropout, training=self.training) # molecule Embedding\n",
    "        \n",
    "        x = self.out(global_add_pool(x, batch))\n",
    "        return x\n",
    "    \n",
    "class AttentiveFPdebug2(torch.nn.Module):\n",
    "    def __init__(self, atom_in_dim, edge_in_dim, fingerprint_dim=200, R=1, T=1, dropout=0.2,  debug = True, outdim=1):\n",
    "        super(AttentiveFPdebug2, self).__init__()\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.dropout = dropout\n",
    "        # call the atom embedding Phase\n",
    "        self.convsAtom = AtomEmbedding2(atom_in_dim, fingerprint_dim, R, debug) \n",
    "        self.convsMol = MoleculeEmbedding(fingerprint_dim, dropout, debug, T )\n",
    "\n",
    "        # fast down project could be much more sofisticated! (ie  Feed Forward Network with multiple layers )\n",
    "        self.out = Linear(fingerprint_dim, outdim) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        x = F.dropout(self.convsAtom(x, edge_index), p=self.dropout, training=self.training) # atom Embedding       \n",
    "        x = F.dropout(self.convsMol(x, edge_index), p=self.dropout, training=self.training) # molecule Embedding\n",
    "        \n",
    "        x = self.out(global_add_pool(x, batch))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveFP(torch.nn.Module):\n",
    "    def __init__(self, atom_in_dim, edge_in_dim, fingerprint_dim=32, R=2, T=2, dropout=0.2,  debug = False, outdim=1):\n",
    "        super(AttentiveFP, self).__init__()\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.dropout = dropout\n",
    "        # call the atom embedding Phase\n",
    "        self.convsAtom = AtomEmbedding(atom_in_dim, edge_in_dim, fingerprint_dim, R, debug)\n",
    "        # call the Mol embedding Phase\n",
    "        self.convsMol = MoleculeEmbedding(fingerprint_dim, dropout, debug, T )\n",
    "\n",
    "        # fast down project could be much more sofisticated! (ie  Feed Forward Network with multiple layers )\n",
    "        self.out = Linear(fingerprint_dim, outdim) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_feat\n",
    "        #x = F.dropout(self.convsAtom(x, edge_index, edge_attr), p=self.dropout, training=self.training) # atom Embedding\n",
    "        x = F.dropout(self.convsMol(x, edge_index), p=self.dropout, training=self.training) # mol Embedding\n",
    "        x = self.out(F.dropout(x, p=self.dropout, training=self.training)) # final prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the model architecture\n",
    "model = AttentiveFPdebug(40, 10, 50, R= 1, T=3, dropout = 0.2, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentiveFPdebug(\n",
       "  (convsAtom): AtomEmbedding(\n",
       "    (conv): ModuleList(\n",
       "      (0): GatConvAtom(\n",
       "        (atom_fc): Linear(in_features=40, out_features=50, bias=True)\n",
       "        (neighbor_fc): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (align): Linear(in_features=100, out_features=1, bias=True)\n",
       "        (attend): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (dropout): Dropout(p=True, inplace=False)\n",
       "        (rnn): GRUCell(50, 50)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (convsMol): MoleculeEmbedding(\n",
       "    (conv): ModuleList(\n",
       "      (0): GatConvMol(\n",
       "        (mol_align): Linear(in_features=100, out_features=1, bias=True)\n",
       "        (mol_attend): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (rnn): GRUCell(50, 50)\n",
       "      )\n",
       "      (1): GatConvMol(\n",
       "        (mol_align): Linear(in_features=100, out_features=1, bias=True)\n",
       "        (mol_attend): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (rnn): GRUCell(50, 50)\n",
       "      )\n",
       "      (2): GatConvMol(\n",
       "        (mol_align): Linear(in_features=100, out_features=1, bias=True)\n",
       "        (mol_attend): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (rnn): GRUCell(50, 50)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[1640], edge_attr=[3380, 10], edge_index=[2, 3380], x=[1640, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3380]) ,x_i: torch.Size([3380, 50])\n",
      "1 mol_feature expanded torch.Size([3380, 50])\n",
      "2 activated_features torch.Size([3380, 50])\n",
      "3 mol_align_score: torch.Size([3380, 1])\n",
      "4 mol_align_score: torch.Size([3380, 1])\n",
      "5 mol_context torch.Size([3380, 50])\n",
      "2 activated_features torch.Size([3380, 50])\n",
      "3 mol_align_score: torch.Size([3380, 1])\n",
      "4 mol_align_score: torch.Size([3380, 1])\n",
      "5 mol_context torch.Size([3380, 50])\n",
      "2 activated_features torch.Size([3380, 50])\n",
      "3 mol_align_score: torch.Size([3380, 1])\n",
      "4 mol_align_score: torch.Size([3380, 1])\n",
      "5 mol_context torch.Size([3380, 50])\n",
      "Batch(batch=[1809], edge_attr=[3742, 10], edge_index=[2, 3742], x=[1809, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3742]) ,x_i: torch.Size([3742, 50])\n",
      "1 mol_feature expanded torch.Size([3742, 50])\n",
      "2 activated_features torch.Size([3742, 50])\n",
      "3 mol_align_score: torch.Size([3742, 1])\n",
      "4 mol_align_score: torch.Size([3742, 1])\n",
      "5 mol_context torch.Size([3742, 50])\n",
      "2 activated_features torch.Size([3742, 50])\n",
      "3 mol_align_score: torch.Size([3742, 1])\n",
      "4 mol_align_score: torch.Size([3742, 1])\n",
      "5 mol_context torch.Size([3742, 50])\n",
      "2 activated_features torch.Size([3742, 50])\n",
      "3 mol_align_score: torch.Size([3742, 1])\n",
      "4 mol_align_score: torch.Size([3742, 1])\n",
      "5 mol_context torch.Size([3742, 50])\n",
      "Batch(batch=[1690], edge_attr=[3480, 10], edge_index=[2, 3480], x=[1690, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3480]) ,x_i: torch.Size([3480, 50])\n",
      "1 mol_feature expanded torch.Size([3480, 50])\n",
      "2 activated_features torch.Size([3480, 50])\n",
      "3 mol_align_score: torch.Size([3480, 1])\n",
      "4 mol_align_score: torch.Size([3480, 1])\n",
      "5 mol_context torch.Size([3480, 50])\n",
      "2 activated_features torch.Size([3480, 50])\n",
      "3 mol_align_score: torch.Size([3480, 1])\n",
      "4 mol_align_score: torch.Size([3480, 1])\n",
      "5 mol_context torch.Size([3480, 50])\n",
      "2 activated_features torch.Size([3480, 50])\n",
      "3 mol_align_score: torch.Size([3480, 1])\n",
      "4 mol_align_score: torch.Size([3480, 1])\n",
      "5 mol_context torch.Size([3480, 50])\n",
      "Batch(batch=[1741], edge_attr=[3618, 10], edge_index=[2, 3618], x=[1741, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3618]) ,x_i: torch.Size([3618, 50])\n",
      "1 mol_feature expanded torch.Size([3618, 50])\n",
      "2 activated_features torch.Size([3618, 50])\n",
      "3 mol_align_score: torch.Size([3618, 1])\n",
      "4 mol_align_score: torch.Size([3618, 1])\n",
      "5 mol_context torch.Size([3618, 50])\n",
      "2 activated_features torch.Size([3618, 50])\n",
      "3 mol_align_score: torch.Size([3618, 1])\n",
      "4 mol_align_score: torch.Size([3618, 1])\n",
      "5 mol_context torch.Size([3618, 50])\n",
      "2 activated_features torch.Size([3618, 50])\n",
      "3 mol_align_score: torch.Size([3618, 1])\n",
      "4 mol_align_score: torch.Size([3618, 1])\n",
      "5 mol_context torch.Size([3618, 50])\n",
      "Batch(batch=[1722], edge_attr=[3556, 10], edge_index=[2, 3556], x=[1722, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3556]) ,x_i: torch.Size([3556, 50])\n",
      "1 mol_feature expanded torch.Size([3556, 50])\n",
      "2 activated_features torch.Size([3556, 50])\n",
      "3 mol_align_score: torch.Size([3556, 1])\n",
      "4 mol_align_score: torch.Size([3556, 1])\n",
      "5 mol_context torch.Size([3556, 50])\n",
      "2 activated_features torch.Size([3556, 50])\n",
      "3 mol_align_score: torch.Size([3556, 1])\n",
      "4 mol_align_score: torch.Size([3556, 1])\n",
      "5 mol_context torch.Size([3556, 50])\n",
      "2 activated_features torch.Size([3556, 50])\n",
      "3 mol_align_score: torch.Size([3556, 1])\n",
      "4 mol_align_score: torch.Size([3556, 1])\n",
      "5 mol_context torch.Size([3556, 50])\n",
      "Batch(batch=[1637], edge_attr=[3354, 10], edge_index=[2, 3354], x=[1637, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3354]) ,x_i: torch.Size([3354, 50])\n",
      "1 mol_feature expanded torch.Size([3354, 50])\n",
      "2 activated_features torch.Size([3354, 50])\n",
      "3 mol_align_score: torch.Size([3354, 1])\n",
      "4 mol_align_score: torch.Size([3354, 1])\n",
      "5 mol_context torch.Size([3354, 50])\n",
      "2 activated_features torch.Size([3354, 50])\n",
      "3 mol_align_score: torch.Size([3354, 1])\n",
      "4 mol_align_score: torch.Size([3354, 1])\n",
      "5 mol_context torch.Size([3354, 50])\n",
      "2 activated_features torch.Size([3354, 50])\n",
      "3 mol_align_score: torch.Size([3354, 1])\n",
      "4 mol_align_score: torch.Size([3354, 1])\n",
      "5 mol_context torch.Size([3354, 50])\n",
      "Batch(batch=[1754], edge_attr=[3622, 10], edge_index=[2, 3622], x=[1754, 40], y=[128, 1])\n",
      "pre-h_s: torch.Size([3622]) ,x_i: torch.Size([3622, 50])\n",
      "1 mol_feature expanded torch.Size([3622, 50])\n",
      "2 activated_features torch.Size([3622, 50])\n",
      "3 mol_align_score: torch.Size([3622, 1])\n",
      "4 mol_align_score: torch.Size([3622, 1])\n",
      "5 mol_context torch.Size([3622, 50])\n",
      "2 activated_features torch.Size([3622, 50])\n",
      "3 mol_align_score: torch.Size([3622, 1])\n",
      "4 mol_align_score: torch.Size([3622, 1])\n",
      "5 mol_context torch.Size([3622, 50])\n",
      "2 activated_features torch.Size([3622, 50])\n",
      "3 mol_align_score: torch.Size([3622, 1])\n",
      "4 mol_align_score: torch.Size([3622, 1])\n",
      "5 mol_context torch.Size([3622, 50])\n",
      "Batch(batch=[1509], edge_attr=[3072, 10], edge_index=[2, 3072], x=[1509, 40], y=[119, 1])\n",
      "pre-h_s: torch.Size([3072]) ,x_i: torch.Size([3072, 50])\n",
      "1 mol_feature expanded torch.Size([3072, 50])\n",
      "2 activated_features torch.Size([3072, 50])\n",
      "3 mol_align_score: torch.Size([3072, 1])\n",
      "4 mol_align_score: torch.Size([3072, 1])\n",
      "5 mol_context torch.Size([3072, 50])\n",
      "2 activated_features torch.Size([3072, 50])\n",
      "3 mol_align_score: torch.Size([3072, 1])\n",
      "4 mol_align_score: torch.Size([3072, 1])\n",
      "5 mol_context torch.Size([3072, 50])\n",
      "2 activated_features torch.Size([3072, 50])\n",
      "3 mol_align_score: torch.Size([3072, 1])\n",
      "4 mol_align_score: torch.Size([3072, 1])\n",
      "5 mol_context torch.Size([3072, 50])\n",
      "0.43885254859924316\n"
     ]
    }
   ],
   "source": [
    "# loop over data in a batch\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y_true = []\n",
    "y_out = []\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    y = model(data)\n",
    "    y_out.extend(y.squeeze().detach().cpu().numpy())\n",
    "    y_true.extend(data.y.squeeze().detach().cpu().numpy())\n",
    "    #break\n",
    "    \n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print                                             \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "################ optimizer #####################\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, R , \n",
    "                 T, \n",
    "                 atom_input_dim = 49,\n",
    "                 bond_input_dim = 10,\n",
    "                 dropout = 0.0,\n",
    "                 fpdim= 200,\n",
    "                 output_dim = 1, \n",
    "                 device=0,\n",
    "                 cuda=True,\n",
    "                 debug = False):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.atom_input_dim = atom_input_dim\n",
    "        self.bond_input_dim = bond_input_dim\n",
    "        self.dropout = dropout\n",
    "        self.fpdim = fpdim\n",
    "        self.debug = debug\n",
    "        self.output_dim = output_dim\n",
    "        self.outputreal=self.output_dim \n",
    "\n",
    "        # device\n",
    "        if cuda:\n",
    "            self.device = torch.device('cuda:%i' %device)\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        self.model = AttentiveFPdebug(self.atom_input_dim,\n",
    "                            self.bond_input_dim,\n",
    "                            self.fpdim,\n",
    "                            self.R,\n",
    "                            self.T, \n",
    "                            self.dropout, \n",
    "                            self.debug)\n",
    "        \n",
    "    def fit(self, data_train,\n",
    "            data_valid,\n",
    "            epochs,\n",
    "            loss,\n",
    "            path = '',\n",
    "            learning_rate = 5e-3,\n",
    "            reducelr = True,\n",
    "            reducefactor = 0.8,\n",
    "            early = False,\n",
    "            patience_early = 40,\n",
    "            patience = 60,\n",
    "            cosine = True,\n",
    "            cosineT = 20,\n",
    "            seed=None,\n",
    "            verbose=True,\n",
    "            logfile=None,\n",
    "            isradam = False,\n",
    "            weight_decay = 0,\n",
    "            amsgrad = False,\n",
    "            best = True):\n",
    "\n",
    "        # check that data is a dataloader\n",
    "        self.path = path\n",
    "        self.nboutputfull = self.outputreal\n",
    "        \n",
    "        if seed and seed >=0:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            \n",
    "        # right away initialize the fit by saving the model\n",
    "        self.save_model(self.path, weights=True)\n",
    "\n",
    "        if isradam:\n",
    "            print('Use RAdam')\n",
    "            opt = RAdam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else: \n",
    "            print('Use Adam')\n",
    "            opt = torch.optim.Adam(self.model.parameters(), lr=learning_rate, amsgrad=amsgrad, weight_decay=weight_decay)\n",
    "\n",
    "            \n",
    "        # reduce on plateau\n",
    "        if reducelr:\n",
    "            plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=patience, factor=reducefactor, verbose=False)\n",
    "                    \n",
    "        # cosine annealing\n",
    "        if cosine:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt,T_0=cosineT, T_mult=2) \n",
    "\n",
    "        # adding early stopping\n",
    "        if early:\n",
    "            early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "            \n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # default parameters if we save the best model\n",
    "        best_loss_val = 3e+6\n",
    "\n",
    "        for t in range(0,epochs):\n",
    "            if logfile is not None:\n",
    "                mode = 'a+'\n",
    "                with open(logfile, mode) as f:\n",
    "                    f.write(\"Epoch {0} / {1}\".format(t+1, epochs))\n",
    "            if verbose:\n",
    "                print(\"Epoch {0} / {1}\".format(t+1, epochs))\n",
    "            epoch_loss = 0.0\n",
    "            count_train = 0\n",
    "            \n",
    "            y_true = torch.tensor([], device=self.device)\n",
    "            y_out = torch.tensor([], device=self.device)\n",
    "            for batch_i, batch_data in enumerate(data_train):\n",
    "                self.model.train()\n",
    "                batch_data = batch_data.to(self.device)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "               \n",
    "                y_batch_pred = self.model(batch_data)\n",
    "                \n",
    "                y_true = torch.cat((y_true,),0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_loss = 0\n",
    "\n",
    "                # still need to handle correctly multitarget\n",
    "                y_out = torch.cat((y_out, y_batch_pred),0)\n",
    "\n",
    "                y_true_tensor = torch.tensor([], device=self.device)\n",
    "                for k in range(self.outputreal): # nboutputfull\n",
    "                    y_true_tensor = torch.cat(( y_true_tensor,batch_data.y.unsqueeze(-1)),1)\n",
    "                # only count std and avg predictions not the rest of the outputs \n",
    "                for k in range(self.outputreal):\n",
    "                    batch_loss += loss(y_batch_pred[:,k].view(-1),batch_data.y)\n",
    "                y_true = torch.cat(( y_true,y_true_tensor),0)\n",
    "\n",
    "                # classical method backward and one step optimizer\n",
    "                batch_loss.backward()\n",
    "                # avoid \n",
    "                \n",
    "                opt.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                loss_train = loss(y_out,y_true).detach().cpu()\n",
    "\n",
    "            val_loss_all = self.predict(data_valid, loss, verbose=0) \n",
    "            val_loss = np.sum(val_loss_all) # change mean to sum for val loss like for train loss\n",
    "                    \n",
    "            if val_loss < best_loss_val: # the mean is taken to consider the multi target case as well\n",
    "                best_loss_val = val_loss\n",
    "                                    \n",
    "            if verbose:\n",
    "                print(\"Train:\",loss_train,\" ,Valid:\",val_loss_all,\" ,Best Loss Val:\",best_loss_val)\n",
    "\n",
    "            if logfile is not None:\n",
    "                with open(logfile, 'a+') as f:\n",
    "                    f.write('\\tTrain: %f\\tVals:' %loss_train)\n",
    "                    for vla in val_loss_all:\n",
    "                        f.write(str(vla))\n",
    "                        f.write('\\t')\n",
    "                    f.write('\\n')\n",
    "                \n",
    "            # compute the early stopping \n",
    "            if early:\n",
    "                early_stopping(val_loss, self.model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "                \n",
    "            # reduce on Plateau\n",
    "            if reducelr:\n",
    "                plateau.step(val_loss)\n",
    "        \n",
    "        self.save_model(self.path, weights=True)\n",
    "\n",
    "    def predict(self, data, loss=None, verbose=1):\n",
    "        self.model.eval()\n",
    "        epoch_loss = {}\n",
    "\n",
    "        for k in range(self.outputreal):\n",
    "            epoch_loss[k] = 0\n",
    "                \n",
    "        count_train = 0\n",
    "\n",
    "        output_augm_vec = []\n",
    "        ytrue_augm_vec = []\n",
    "        augm_idx_vec = []\n",
    "\n",
    "        # initialize the loss vecs\n",
    "        pred_loss_vec = []\n",
    "        true_loss_vec = []\n",
    "        \n",
    "        for batch_data in data:\n",
    "            batch_data = batch_data.to(self.device)\n",
    "            y_batch_pred = self.model(batch_data)\n",
    "            \n",
    "            #print(y_batch_pred.shape)\n",
    "            #print(batch_data.y.shape)\n",
    "        \n",
    "            batch_loss = {}\n",
    "            count_train += batch_data.y.size(0)\n",
    "            for k in range(self.outputreal):\n",
    "                # scaled data\n",
    "                if k==0:\n",
    "                    batch_loss[k] = loss(y_batch_pred[:,k].view(-1), batch_data.y)\n",
    "                if k>0:\n",
    "                    batch_loss[k] = loss(y_batch_pred[:,k].view(-1),  batch_data.y)\n",
    "                epoch_loss[k] += batch_data.num_graphs * batch_loss[k].item()\n",
    "        \n",
    "        return [epoch_loss[k]/count_train for k in range(self.outputreal)]\n",
    "    \n",
    "    def apply_model(self, data):\n",
    "        self.model.eval()\n",
    "\n",
    "        # initialize the loss vecs\n",
    "        y_out = torch.tensor([], device=self.device)\n",
    "        y_true = torch.tensor([], device=self.device)\n",
    "\n",
    "        for batch_data in data:\n",
    "            y_true_tensor = torch.tensor([], device=self.device)\n",
    "            batch_data = batch_data.to(self.device)\n",
    "            y_batch_pred = self.model(batch_data)\n",
    "            \n",
    "            # still need to handle correctly multitarget\n",
    "            y_out = torch.cat((y_out, y_batch_pred),0)\n",
    "            for k in range(self.outputreal):\n",
    "                y_true_tensor = torch.cat(( y_true_tensor,(batch_data.y).unsqueeze(-1)),1)\n",
    "            y_true = torch.cat(( y_true,y_true_tensor),0)\n",
    "        return y_out , y_true  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_model(self, path, weights=False):\n",
    "        \"\"\" \n",
    "        method to save the trained model\n",
    "        :param path: the full path where to save the model, str\n",
    "        :param weights: boolean, whether to consider only the weights, otherwise the full model (archi+weights)\n",
    "        \"\"\"\n",
    "        if weights:\n",
    "            torch.save(self.model.cpu().state_dict(), path)\n",
    "            self.model = self.model.to(self.device)\n",
    "        else:\n",
    "            torch.save(self.model.cpu(), path)\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "            \n",
    "    def load_model(self, path, weights=False):\n",
    "        \"\"\" \n",
    "        method to save the trained model\n",
    "        :param path: the full path from where to load the model, str\n",
    "        :param weights: boolean, whether to consider only the weights, otherwise the full model (archi+weights)\n",
    "        \"\"\"\n",
    "        if weights:\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "            self.model = self.model.to(self.device)\n",
    "        else:\n",
    "            self.model.load(path)\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [00:06<00:00, 162.32it/s]\n",
      "100%|██████████| 113/113 [00:00<00:00, 171.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Model object at 0x7fcced4a6080>\n",
      "Use Adam\n",
      "Epoch 1 / 200\n",
      "Train: tensor(377.7852)  ,Valid: [17.907276153564453]  ,Best Loss Val: 17.907276153564453\n",
      "Validation loss decreased (inf --> 17.907276).  Saving model ...\n",
      "Epoch 2 / 200\n",
      "Train: tensor(56.4372)  ,Valid: [11.113434791564941]  ,Best Loss Val: 11.113434791564941\n",
      "Validation loss decreased (17.907276 --> 11.113435).  Saving model ...\n",
      "Epoch 3 / 200\n",
      "Train: tensor(22.6941)  ,Valid: [9.732998847961426]  ,Best Loss Val: 9.732998847961426\n",
      "Validation loss decreased (11.113435 --> 9.732999).  Saving model ...\n",
      "Epoch 4 / 200\n",
      "Train: tensor(16.8542)  ,Valid: [15.343832969665527]  ,Best Loss Val: 9.732998847961426\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 5 / 200\n",
      "Train: tensor(14.7905)  ,Valid: [6.593906879425049]  ,Best Loss Val: 6.593906879425049\n",
      "Validation loss decreased (9.732999 --> 6.593907).  Saving model ...\n",
      "Epoch 6 / 200\n",
      "Train: tensor(10.9401)  ,Valid: [5.8416523933410645]  ,Best Loss Val: 5.8416523933410645\n",
      "Validation loss decreased (6.593907 --> 5.841652).  Saving model ...\n",
      "Epoch 7 / 200\n",
      "Train: tensor(10.3309)  ,Valid: [4.831771373748779]  ,Best Loss Val: 4.831771373748779\n",
      "Validation loss decreased (5.841652 --> 4.831771).  Saving model ...\n",
      "Epoch 8 / 200\n",
      "Train: tensor(8.8001)  ,Valid: [5.596650123596191]  ,Best Loss Val: 4.831771373748779\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 9 / 200\n",
      "Train: tensor(7.7091)  ,Valid: [3.0055580139160156]  ,Best Loss Val: 3.0055580139160156\n",
      "Validation loss decreased (4.831771 --> 3.005558).  Saving model ...\n",
      "Epoch 10 / 200\n",
      "Train: tensor(7.1428)  ,Valid: [4.685544967651367]  ,Best Loss Val: 3.0055580139160156\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 11 / 200\n",
      "Train: tensor(6.3703)  ,Valid: [3.350050687789917]  ,Best Loss Val: 3.0055580139160156\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 12 / 200\n",
      "Train: tensor(6.0989)  ,Valid: [2.586090564727783]  ,Best Loss Val: 2.586090564727783\n",
      "Validation loss decreased (3.005558 --> 2.586091).  Saving model ...\n",
      "Epoch 13 / 200\n",
      "Train: tensor(5.8004)  ,Valid: [3.5902836322784424]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 14 / 200\n",
      "Train: tensor(6.2845)  ,Valid: [4.487175941467285]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 15 / 200\n",
      "Train: tensor(5.4874)  ,Valid: [3.097075939178467]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 16 / 200\n",
      "Train: tensor(5.7871)  ,Valid: [4.847127914428711]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 17 / 200\n",
      "Train: tensor(6.1704)  ,Valid: [3.1354382038116455]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 18 / 200\n",
      "Train: tensor(6.1068)  ,Valid: [3.1024131774902344]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 19 / 200\n",
      "Train: tensor(5.3738)  ,Valid: [4.198421955108643]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 20 / 200\n",
      "Train: tensor(5.3665)  ,Valid: [2.7572386264801025]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 21 / 200\n",
      "Train: tensor(5.4416)  ,Valid: [4.29340934753418]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 22 / 200\n",
      "Train: tensor(5.3586)  ,Valid: [3.392854690551758]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 10 out of 40\n",
      "Epoch 23 / 200\n",
      "Train: tensor(5.2856)  ,Valid: [2.666743278503418]  ,Best Loss Val: 2.586090564727783\n",
      "EarlyStopping counter: 11 out of 40\n",
      "Epoch 24 / 200\n",
      "Train: tensor(5.5005)  ,Valid: [2.3518989086151123]  ,Best Loss Val: 2.3518989086151123\n",
      "Validation loss decreased (2.586091 --> 2.351899).  Saving model ...\n",
      "Epoch 25 / 200\n",
      "Train: tensor(5.3587)  ,Valid: [2.524930238723755]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 26 / 200\n",
      "Train: tensor(4.7984)  ,Valid: [3.23664927482605]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 27 / 200\n",
      "Train: tensor(4.7813)  ,Valid: [4.5569682121276855]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 28 / 200\n",
      "Train: tensor(4.7920)  ,Valid: [2.546842098236084]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 29 / 200\n",
      "Train: tensor(4.6726)  ,Valid: [3.5686898231506348]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 30 / 200\n",
      "Train: tensor(4.4835)  ,Valid: [2.6845908164978027]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 31 / 200\n",
      "Train: tensor(3.9995)  ,Valid: [2.653806447982788]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 32 / 200\n",
      "Train: tensor(4.1317)  ,Valid: [2.8069653511047363]  ,Best Loss Val: 2.3518989086151123\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 33 / 200\n",
      "Train: tensor(3.7008)  ,Valid: [2.2865312099456787]  ,Best Loss Val: 2.2865312099456787\n",
      "Validation loss decreased (2.351899 --> 2.286531).  Saving model ...\n",
      "Epoch 34 / 200\n",
      "Train: tensor(3.7628)  ,Valid: [2.4472053050994873]  ,Best Loss Val: 2.2865312099456787\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 35 / 200\n",
      "Train: tensor(3.6772)  ,Valid: [2.450371265411377]  ,Best Loss Val: 2.2865312099456787\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 36 / 200\n",
      "Train: tensor(3.7857)  ,Valid: [2.226724624633789]  ,Best Loss Val: 2.226724624633789\n",
      "Validation loss decreased (2.286531 --> 2.226725).  Saving model ...\n",
      "Epoch 37 / 200\n",
      "Train: tensor(3.4336)  ,Valid: [2.235170364379883]  ,Best Loss Val: 2.226724624633789\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 38 / 200\n",
      "Train: tensor(3.5840)  ,Valid: [3.687331438064575]  ,Best Loss Val: 2.226724624633789\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 39 / 200\n",
      "Train: tensor(3.7024)  ,Valid: [3.0077521800994873]  ,Best Loss Val: 2.226724624633789\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 40 / 200\n",
      "Train: tensor(3.4611)  ,Valid: [2.1865177154541016]  ,Best Loss Val: 2.1865177154541016\n",
      "Validation loss decreased (2.226725 --> 2.186518).  Saving model ...\n",
      "Epoch 41 / 200\n",
      "Train: tensor(3.1802)  ,Valid: [2.515544891357422]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 42 / 200\n",
      "Train: tensor(3.5131)  ,Valid: [3.729106903076172]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 43 / 200\n",
      "Train: tensor(3.7511)  ,Valid: [2.2337961196899414]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 44 / 200\n",
      "Train: tensor(3.2635)  ,Valid: [2.5327188968658447]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 45 / 200\n",
      "Train: tensor(3.1625)  ,Valid: [2.3468666076660156]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 46 / 200\n",
      "Train: tensor(3.0889)  ,Valid: [2.981834650039673]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 47 / 200\n",
      "Train: tensor(3.2328)  ,Valid: [2.9598822593688965]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 48 / 200\n",
      "Train: tensor(3.1534)  ,Valid: [2.421828269958496]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 49 / 200\n",
      "Train: tensor(3.1987)  ,Valid: [2.256788492202759]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 50 / 200\n",
      "Train: tensor(2.9436)  ,Valid: [2.298628091812134]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 10 out of 40\n",
      "Epoch 51 / 200\n",
      "Train: tensor(2.9291)  ,Valid: [2.263812780380249]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 11 out of 40\n",
      "Epoch 52 / 200\n",
      "Train: tensor(2.8679)  ,Valid: [2.1947455406188965]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 12 out of 40\n",
      "Epoch 53 / 200\n",
      "Train: tensor(2.8966)  ,Valid: [2.7551605701446533]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 13 out of 40\n",
      "Epoch 54 / 200\n",
      "Train: tensor(2.9516)  ,Valid: [2.1925101280212402]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 14 out of 40\n",
      "Epoch 55 / 200\n",
      "Train: tensor(2.7982)  ,Valid: [2.1874735355377197]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 15 out of 40\n",
      "Epoch 56 / 200\n",
      "Train: tensor(2.9204)  ,Valid: [2.3826568126678467]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 16 out of 40\n",
      "Epoch 57 / 200\n",
      "Train: tensor(2.8619)  ,Valid: [2.3696188926696777]  ,Best Loss Val: 2.1865177154541016\n",
      "EarlyStopping counter: 17 out of 40\n",
      "Epoch 58 / 200\n",
      "Train: tensor(2.8614)  ,Valid: [2.1741857528686523]  ,Best Loss Val: 2.1741857528686523\n",
      "Validation loss decreased (2.186518 --> 2.174186).  Saving model ...\n",
      "Epoch 59 / 200\n",
      "Train: tensor(2.9278)  ,Valid: [2.573474645614624]  ,Best Loss Val: 2.1741857528686523\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 60 / 200\n",
      "Train: tensor(2.6950)  ,Valid: [2.182739019393921]  ,Best Loss Val: 2.1741857528686523\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 61 / 200\n",
      "Train: tensor(2.6952)  ,Valid: [2.2935659885406494]  ,Best Loss Val: 2.1741857528686523\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 62 / 200\n",
      "Train: tensor(2.8367)  ,Valid: [2.8350274562835693]  ,Best Loss Val: 2.1741857528686523\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 63 / 200\n",
      "Train: tensor(3.0043)  ,Valid: [2.141932964324951]  ,Best Loss Val: 2.141932964324951\n",
      "Validation loss decreased (2.174186 --> 2.141933).  Saving model ...\n",
      "Epoch 64 / 200\n",
      "Train: tensor(2.7570)  ,Valid: [2.509490489959717]  ,Best Loss Val: 2.141932964324951\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 65 / 200\n",
      "Train: tensor(2.6900)  ,Valid: [2.23531174659729]  ,Best Loss Val: 2.141932964324951\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 66 / 200\n",
      "Train: tensor(2.6572)  ,Valid: [2.2717432975769043]  ,Best Loss Val: 2.141932964324951\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 67 / 200\n",
      "Train: tensor(2.7600)  ,Valid: [2.164792060852051]  ,Best Loss Val: 2.141932964324951\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 68 / 200\n",
      "Train: tensor(2.7255)  ,Valid: [2.22223162651062]  ,Best Loss Val: 2.141932964324951\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 69 / 200\n",
      "Train: tensor(2.7755)  ,Valid: [2.0959160327911377]  ,Best Loss Val: 2.0959160327911377\n",
      "Validation loss decreased (2.141933 --> 2.095916).  Saving model ...\n",
      "Epoch 70 / 200\n",
      "Train: tensor(2.8935)  ,Valid: [2.2881009578704834]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 71 / 200\n",
      "Train: tensor(2.6734)  ,Valid: [2.4057180881500244]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 72 / 200\n",
      "Train: tensor(2.5406)  ,Valid: [2.263376474380493]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 73 / 200\n",
      "Train: tensor(2.7515)  ,Valid: [2.366433620452881]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 74 / 200\n",
      "Train: tensor(2.6483)  ,Valid: [2.3396289348602295]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 75 / 200\n",
      "Train: tensor(2.6400)  ,Valid: [2.396332263946533]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 76 / 200\n",
      "Train: tensor(2.5400)  ,Valid: [2.178537368774414]  ,Best Loss Val: 2.0959160327911377\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 77 / 200\n",
      "Train: tensor(2.4643)  ,Valid: [2.0946385860443115]  ,Best Loss Val: 2.0946385860443115\n",
      "Validation loss decreased (2.095916 --> 2.094639).  Saving model ...\n",
      "Epoch 78 / 200\n",
      "Train: tensor(2.4986)  ,Valid: [2.101907730102539]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 79 / 200\n",
      "Train: tensor(2.4684)  ,Valid: [2.108513355255127]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 80 / 200\n",
      "Train: tensor(2.5103)  ,Valid: [2.131809711456299]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 81 / 200\n",
      "Train: tensor(2.4374)  ,Valid: [2.0995309352874756]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 82 / 200\n",
      "Train: tensor(2.4535)  ,Valid: [2.1041922569274902]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 83 / 200\n",
      "Train: tensor(2.4725)  ,Valid: [2.21270489692688]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 84 / 200\n",
      "Train: tensor(2.4571)  ,Valid: [2.4578359127044678]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 85 / 200\n",
      "Train: tensor(2.4314)  ,Valid: [2.135533571243286]  ,Best Loss Val: 2.0946385860443115\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 86 / 200\n",
      "Train: tensor(2.4510)  ,Valid: [2.074185609817505]  ,Best Loss Val: 2.074185609817505\n",
      "Validation loss decreased (2.094639 --> 2.074186).  Saving model ...\n",
      "Epoch 87 / 200\n",
      "Train: tensor(2.4505)  ,Valid: [2.163313865661621]  ,Best Loss Val: 2.074185609817505\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 88 / 200\n",
      "Train: tensor(2.3690)  ,Valid: [2.0976784229278564]  ,Best Loss Val: 2.074185609817505\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 89 / 200\n",
      "Train: tensor(2.3746)  ,Valid: [2.0862843990325928]  ,Best Loss Val: 2.074185609817505\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 90 / 200\n",
      "Train: tensor(2.3935)  ,Valid: [2.088250160217285]  ,Best Loss Val: 2.074185609817505\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 91 / 200\n",
      "Train: tensor(2.3791)  ,Valid: [2.12514328956604]  ,Best Loss Val: 2.074185609817505\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 92 / 200\n",
      "Train: tensor(2.4023)  ,Valid: [2.1080620288848877]  ,Best Loss Val: 2.074185609817505\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 93 / 200\n",
      "Train: tensor(2.3821)  ,Valid: [2.0638864040374756]  ,Best Loss Val: 2.0638864040374756\n",
      "Validation loss decreased (2.074186 --> 2.063886).  Saving model ...\n",
      "Epoch 94 / 200\n",
      "Train: tensor(2.3701)  ,Valid: [2.0931129455566406]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 95 / 200\n",
      "Train: tensor(2.4113)  ,Valid: [2.2606565952301025]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 96 / 200\n",
      "Train: tensor(2.4054)  ,Valid: [2.229445457458496]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 97 / 200\n",
      "Train: tensor(2.3762)  ,Valid: [2.4937121868133545]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 98 / 200\n",
      "Train: tensor(2.3776)  ,Valid: [2.642949342727661]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 99 / 200\n",
      "Train: tensor(2.5234)  ,Valid: [2.240692615509033]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 100 / 200\n",
      "Train: tensor(2.4826)  ,Valid: [2.888775587081909]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 101 / 200\n",
      "Train: tensor(2.6075)  ,Valid: [2.212029457092285]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 102 / 200\n",
      "Train: tensor(2.4513)  ,Valid: [2.081665277481079]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 103 / 200\n",
      "Train: tensor(2.3666)  ,Valid: [2.249553918838501]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 10 out of 40\n",
      "Epoch 104 / 200\n",
      "Train: tensor(2.3502)  ,Valid: [2.072758674621582]  ,Best Loss Val: 2.0638864040374756\n",
      "EarlyStopping counter: 11 out of 40\n",
      "Epoch 105 / 200\n",
      "Train: tensor(2.3300)  ,Valid: [2.054644823074341]  ,Best Loss Val: 2.054644823074341\n",
      "Validation loss decreased (2.063886 --> 2.054645).  Saving model ...\n",
      "Epoch 106 / 200\n",
      "Train: tensor(2.3217)  ,Valid: [2.0674493312835693]  ,Best Loss Val: 2.054644823074341\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 107 / 200\n",
      "Train: tensor(2.3217)  ,Valid: [2.0615177154541016]  ,Best Loss Val: 2.054644823074341\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 108 / 200\n",
      "Train: tensor(2.3040)  ,Valid: [2.0649940967559814]  ,Best Loss Val: 2.054644823074341\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 109 / 200\n",
      "Train: tensor(2.3389)  ,Valid: [2.061258554458618]  ,Best Loss Val: 2.054644823074341\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 110 / 200\n",
      "Train: tensor(2.3237)  ,Valid: [2.1832633018493652]  ,Best Loss Val: 2.054644823074341\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 111 / 200\n",
      "Train: tensor(2.3149)  ,Valid: [2.052520990371704]  ,Best Loss Val: 2.052520990371704\n",
      "Validation loss decreased (2.054645 --> 2.052521).  Saving model ...\n",
      "Epoch 112 / 200\n",
      "Train: tensor(2.3250)  ,Valid: [2.2796902656555176]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 113 / 200\n",
      "Train: tensor(2.3404)  ,Valid: [2.147595167160034]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 114 / 200\n",
      "Train: tensor(2.3282)  ,Valid: [2.095061779022217]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 115 / 200\n",
      "Train: tensor(2.3864)  ,Valid: [2.112806558609009]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 116 / 200\n",
      "Train: tensor(2.3693)  ,Valid: [2.0577287673950195]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 117 / 200\n",
      "Train: tensor(2.3454)  ,Valid: [2.1784210205078125]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 118 / 200\n",
      "Train: tensor(2.2956)  ,Valid: [2.264495372772217]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 119 / 200\n",
      "Train: tensor(2.3073)  ,Valid: [2.2161500453948975]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 120 / 200\n",
      "Train: tensor(2.2699)  ,Valid: [2.0551700592041016]  ,Best Loss Val: 2.052520990371704\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 121 / 200\n",
      "Train: tensor(2.2757)  ,Valid: [2.034592390060425]  ,Best Loss Val: 2.034592390060425\n",
      "Validation loss decreased (2.052521 --> 2.034592).  Saving model ...\n",
      "Epoch 122 / 200\n",
      "Train: tensor(2.3199)  ,Valid: [2.1123557090759277]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 123 / 200\n",
      "Train: tensor(2.2772)  ,Valid: [2.0518553256988525]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 124 / 200\n",
      "Train: tensor(2.3005)  ,Valid: [2.0941781997680664]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 125 / 200\n",
      "Train: tensor(2.3023)  ,Valid: [2.0417909622192383]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 126 / 200\n",
      "Train: tensor(2.3021)  ,Valid: [2.1395931243896484]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 127 / 200\n",
      "Train: tensor(2.2745)  ,Valid: [2.464296340942383]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 128 / 200\n",
      "Train: tensor(2.3096)  ,Valid: [2.0375397205352783]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 129 / 200\n",
      "Train: tensor(2.2822)  ,Valid: [2.0359585285186768]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 130 / 200\n",
      "Train: tensor(2.3060)  ,Valid: [2.1013119220733643]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 131 / 200\n",
      "Train: tensor(2.3193)  ,Valid: [2.1620571613311768]  ,Best Loss Val: 2.034592390060425\n",
      "EarlyStopping counter: 10 out of 40\n",
      "Epoch 132 / 200\n",
      "Train: tensor(2.2668)  ,Valid: [2.0317628383636475]  ,Best Loss Val: 2.0317628383636475\n",
      "Validation loss decreased (2.034592 --> 2.031763).  Saving model ...\n",
      "Epoch 133 / 200\n",
      "Train: tensor(2.2405)  ,Valid: [2.0889406204223633]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 134 / 200\n",
      "Train: tensor(2.2367)  ,Valid: [2.086543083190918]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 135 / 200\n",
      "Train: tensor(2.2377)  ,Valid: [2.0559659004211426]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 136 / 200\n",
      "Train: tensor(2.2754)  ,Valid: [2.0374109745025635]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 137 / 200\n",
      "Train: tensor(2.2234)  ,Valid: [2.034593343734741]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 138 / 200\n",
      "Train: tensor(2.2570)  ,Valid: [2.19602108001709]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 139 / 200\n",
      "Train: tensor(2.2233)  ,Valid: [2.10538911819458]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 140 / 200\n",
      "Train: tensor(2.2117)  ,Valid: [2.101269245147705]  ,Best Loss Val: 2.0317628383636475\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 141 / 200\n",
      "Train: tensor(2.2308)  ,Valid: [2.0211164951324463]  ,Best Loss Val: 2.0211164951324463\n",
      "Validation loss decreased (2.031763 --> 2.021116).  Saving model ...\n",
      "Epoch 142 / 200\n",
      "Train: tensor(2.2565)  ,Valid: [2.2497241497039795]  ,Best Loss Val: 2.0211164951324463\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 143 / 200\n",
      "Train: tensor(2.2578)  ,Valid: [2.0206096172332764]  ,Best Loss Val: 2.0206096172332764\n",
      "Validation loss decreased (2.021116 --> 2.020610).  Saving model ...\n",
      "Epoch 144 / 200\n",
      "Train: tensor(2.2141)  ,Valid: [2.1675240993499756]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 145 / 200\n",
      "Train: tensor(2.2233)  ,Valid: [2.043621301651001]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 146 / 200\n",
      "Train: tensor(2.2128)  ,Valid: [2.0426812171936035]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 147 / 200\n",
      "Train: tensor(2.2077)  ,Valid: [2.085941791534424]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 148 / 200\n",
      "Train: tensor(2.1971)  ,Valid: [2.0380566120147705]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 149 / 200\n",
      "Train: tensor(2.2069)  ,Valid: [2.023918628692627]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 150 / 200\n",
      "Train: tensor(2.2378)  ,Valid: [2.062915563583374]  ,Best Loss Val: 2.0206096172332764\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 151 / 200\n",
      "Train: tensor(2.2199)  ,Valid: [2.0148611068725586]  ,Best Loss Val: 2.0148611068725586\n",
      "Validation loss decreased (2.020610 --> 2.014861).  Saving model ...\n",
      "Epoch 152 / 200\n",
      "Train: tensor(2.2449)  ,Valid: [2.0660135746002197]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 153 / 200\n",
      "Train: tensor(2.2162)  ,Valid: [2.078075408935547]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 154 / 200\n",
      "Train: tensor(2.2157)  ,Valid: [2.089437961578369]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 155 / 200\n",
      "Train: tensor(2.2146)  ,Valid: [2.057203531265259]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 156 / 200\n",
      "Train: tensor(2.1934)  ,Valid: [2.0564863681793213]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 157 / 200\n",
      "Train: tensor(2.1851)  ,Valid: [2.2231802940368652]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 158 / 200\n",
      "Train: tensor(2.2077)  ,Valid: [2.0411593914031982]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 159 / 200\n",
      "Train: tensor(2.2031)  ,Valid: [2.062431812286377]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 160 / 200\n",
      "Train: tensor(2.1910)  ,Valid: [2.0317037105560303]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 161 / 200\n",
      "Train: tensor(2.1839)  ,Valid: [2.0582382678985596]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 10 out of 40\n",
      "Epoch 162 / 200\n",
      "Train: tensor(2.2406)  ,Valid: [2.1966192722320557]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 11 out of 40\n",
      "Epoch 163 / 200\n",
      "Train: tensor(2.2696)  ,Valid: [3.150585651397705]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 12 out of 40\n",
      "Epoch 164 / 200\n",
      "Train: tensor(2.3972)  ,Valid: [2.340123176574707]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 13 out of 40\n",
      "Epoch 165 / 200\n",
      "Train: tensor(2.2615)  ,Valid: [2.3282859325408936]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 14 out of 40\n",
      "Epoch 166 / 200\n",
      "Train: tensor(2.2532)  ,Valid: [2.3162841796875]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 15 out of 40\n",
      "Epoch 167 / 200\n",
      "Train: tensor(2.2295)  ,Valid: [2.0794177055358887]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 16 out of 40\n",
      "Epoch 168 / 200\n",
      "Train: tensor(2.2675)  ,Valid: [2.0682313442230225]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 17 out of 40\n",
      "Epoch 169 / 200\n",
      "Train: tensor(2.2894)  ,Valid: [2.0308141708374023]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 18 out of 40\n",
      "Epoch 170 / 200\n",
      "Train: tensor(2.2566)  ,Valid: [2.168445110321045]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 19 out of 40\n",
      "Epoch 171 / 200\n",
      "Train: tensor(2.2457)  ,Valid: [2.1055848598480225]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 20 out of 40\n",
      "Epoch 172 / 200\n",
      "Train: tensor(2.1977)  ,Valid: [2.0612382888793945]  ,Best Loss Val: 2.0148611068725586\n",
      "EarlyStopping counter: 21 out of 40\n",
      "Epoch 173 / 200\n",
      "Train: tensor(2.2142)  ,Valid: [2.0125672817230225]  ,Best Loss Val: 2.0125672817230225\n",
      "Validation loss decreased (2.014861 --> 2.012567).  Saving model ...\n",
      "Epoch 174 / 200\n",
      "Train: tensor(2.2057)  ,Valid: [2.1239469051361084]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 175 / 200\n",
      "Train: tensor(2.2090)  ,Valid: [2.0354201793670654]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 176 / 200\n",
      "Train: tensor(2.2205)  ,Valid: [2.081343173980713]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 177 / 200\n",
      "Train: tensor(2.2718)  ,Valid: [2.0297775268554688]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 178 / 200\n",
      "Train: tensor(2.2160)  ,Valid: [2.3270652294158936]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 179 / 200\n",
      "Train: tensor(2.2058)  ,Valid: [2.0137088298797607]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 180 / 200\n",
      "Train: tensor(2.2237)  ,Valid: [2.028818368911743]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 181 / 200\n",
      "Train: tensor(2.2315)  ,Valid: [2.122683048248291]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 182 / 200\n",
      "Train: tensor(2.2780)  ,Valid: [2.034180164337158]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Epoch 183 / 200\n",
      "Train: tensor(2.1917)  ,Valid: [2.101200580596924]  ,Best Loss Val: 2.0125672817230225\n",
      "EarlyStopping counter: 10 out of 40\n",
      "Epoch 184 / 200\n",
      "Train: tensor(2.2286)  ,Valid: [2.003206729888916]  ,Best Loss Val: 2.003206729888916\n",
      "Validation loss decreased (2.012567 --> 2.003207).  Saving model ...\n",
      "Epoch 185 / 200\n",
      "Train: tensor(2.1827)  ,Valid: [2.001004695892334]  ,Best Loss Val: 2.001004695892334\n",
      "Validation loss decreased (2.003207 --> 2.001005).  Saving model ...\n",
      "Epoch 186 / 200\n",
      "Train: tensor(2.1802)  ,Valid: [2.086653232574463]  ,Best Loss Val: 2.001004695892334\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 187 / 200\n",
      "Train: tensor(2.1961)  ,Valid: [2.146454095840454]  ,Best Loss Val: 2.001004695892334\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 188 / 200\n",
      "Train: tensor(2.1961)  ,Valid: [2.0172605514526367]  ,Best Loss Val: 2.001004695892334\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 189 / 200\n",
      "Train: tensor(2.1823)  ,Valid: [2.0286364555358887]  ,Best Loss Val: 2.001004695892334\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 190 / 200\n",
      "Train: tensor(2.1894)  ,Valid: [2.0146474838256836]  ,Best Loss Val: 2.001004695892334\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 191 / 200\n",
      "Train: tensor(2.1913)  ,Valid: [1.9994056224822998]  ,Best Loss Val: 1.9994056224822998\n",
      "Validation loss decreased (2.001005 --> 1.999406).  Saving model ...\n",
      "Epoch 192 / 200\n",
      "Train: tensor(2.1766)  ,Valid: [2.022923707962036]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 193 / 200\n",
      "Train: tensor(2.1702)  ,Valid: [2.065382242202759]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 2 out of 40\n",
      "Epoch 194 / 200\n",
      "Train: tensor(2.1700)  ,Valid: [1.9997806549072266]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 3 out of 40\n",
      "Epoch 195 / 200\n",
      "Train: tensor(2.1750)  ,Valid: [2.01509165763855]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 4 out of 40\n",
      "Epoch 196 / 200\n",
      "Train: tensor(2.1686)  ,Valid: [2.1489500999450684]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 5 out of 40\n",
      "Epoch 197 / 200\n",
      "Train: tensor(2.1862)  ,Valid: [2.096452236175537]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 6 out of 40\n",
      "Epoch 198 / 200\n",
      "Train: tensor(2.2407)  ,Valid: [2.0379695892333984]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 7 out of 40\n",
      "Epoch 199 / 200\n",
      "Train: tensor(2.1837)  ,Valid: [2.2926018238067627]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 8 out of 40\n",
      "Epoch 200 / 200\n",
      "Train: tensor(2.2631)  ,Valid: [2.0295283794403076]  ,Best Loss Val: 1.9994056224822998\n",
      "EarlyStopping counter: 9 out of 40\n",
      "Relax!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "################## basic parameters ############\n",
    "target_list = ['target']\n",
    "output_dim = 1\n",
    "patience = 40\n",
    "verbose = 1\n",
    "\n",
    "################ OCHEM integration part ###############\n",
    "import configparser\n",
    "import tarfile\n",
    "import pickle\n",
    "config = configparser.ConfigParser();\n",
    "config.read('config-attfp-my.cfg');\n",
    "\n",
    "\n",
    "def getConfig(section, attribute, default=\"\", type=\"int\"):\n",
    "    try:\n",
    "        if type == 'int':\n",
    "            return config.getint(section,attribute,fallback = default);\n",
    "        if type == 'float':\n",
    "            return config.getfloat(section,attribute,fallback = default);\n",
    "        if type == 'bool':\n",
    "            return config.getboolean(section,attribute,fallback = default);\n",
    "        else:\n",
    "            return config.get(section,attribute,fallback = default);\n",
    "\n",
    "    except:\n",
    "        return default;\n",
    "\n",
    "TRAIN = getConfig(\"Task\",\"train_mode\",True,'bool');\n",
    "MODEL_FILE = getConfig(\"Task\",\"model_file\",\"\",\"\");\n",
    "TRAIN_FILE = getConfig(\"Task\",\"train_data_file\" ,\"esol.csv\",\"\");\n",
    "APPLY_FILE = getConfig(\"Task\",\"apply_data_file\", \"train.csv\",\"\");\n",
    "RESULT_FILE = getConfig(\"Task\",\"result_file\", \"results.csv\",\"\");\n",
    "\n",
    "best = getConfig(\"Details\",\"best\",True,'bool');\n",
    "isradam = getConfig(\"Details\",\"Adam\",False,'bool');\n",
    "nbepochs = getConfig(\"Details\", \"nbepochs\", 10,'int');\n",
    "R = getConfig(\"Details\", \"R\", 1,'int');\n",
    "T = getConfig(\"Details\", \"T\", 1,'int');\n",
    "fpdim = getConfig(\"Details\", \"fpdim\", 200,'int');\n",
    "lr = getConfig(\"Details\", \"lr\", 0.001, 'float');\n",
    "batch_size = getConfig(\"Details\", \"batch\", 32, 'int');\n",
    "dropout = getConfig(\"Details\", \"dropout\", 0.2, 'float');\n",
    "seed = getConfig(\"Details\", \"seed\", 420, 'int');\n",
    "cosine =  getConfig(\"Details\", \"cosine\", True,'bool')\n",
    "cosineT = getConfig(\"Details\", \"cosineT\", 14, 'int');\n",
    "reducelr =  getConfig(\"Details\", \"reducelr\",  False,'bool');\n",
    "patience_lr = getConfig(\"Details\", \"patiencelr\", 40,'int');\n",
    "patience_early = getConfig(\"Details\", \"patience_early\", 40, 'int');\n",
    "early = getConfig(\"Details\", \"early\", True, 'bool') \n",
    "gpu = getConfig(\"Details\", \"gpu\", 0, 'int');\n",
    "weight_decay = getConfig(\"Details\", \"weight_decay\", 0.0, 'float');\n",
    "amsgrad = getConfig(\"Details\", \"amsgrad\", False, 'bool')     \n",
    "    \n",
    "log_filename = 'model.log';\n",
    "modelname = \"model.pt\";\n",
    "\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "\n",
    "criterion =  RMSELoss()\n",
    "\n",
    "### need to change to bool now!\n",
    "if TRAIN:\n",
    "\n",
    "   data = [];\n",
    "   first = True;\n",
    "   for line in open(TRAIN_FILE, \"r\").readlines():\n",
    "      if not first:\n",
    "         arr = line.strip().split(\",\");\n",
    "         data.append([arr[0], float(arr[1]) ]);\n",
    "      first = False;\n",
    "\n",
    "   total = len(data);\n",
    "   ntrain = int(0.9 * total);\n",
    "\n",
    "   df_train = pd.DataFrame(data[:ntrain], columns = [\"smiles\", \"target\"] );\n",
    "   df_valid = pd.DataFrame(data[ntrain:], columns = [\"smiles\", \"target\"] );\n",
    "    \n",
    "    \n",
    "   traindata = datagenerator(df_train, target_list) \n",
    "\n",
    "   valdata = datagenerator(df_valid, target_list) \n",
    "    \n",
    "   \n",
    "   train_loader= getdataloader(traindata, batch_size,  shuffle=True,  drop_last=False)\n",
    "    \n",
    "    \n",
    "   valid_loader = getdataloader(valdata, batch_size,  shuffle=False,  drop_last=False)\n",
    "\n",
    "   # need to add classifier ouptut option\n",
    "   model = Model(R=2, T =2, atom_input_dim = 40, bond_input_dim = 10,  dropout=0.3, fpdim=200,  output_dim=1)\n",
    "    \n",
    "   print(model)\n",
    "    \n",
    "   model.fit(train_loader, valid_loader, nbepochs, criterion, path = modelname, cosine=cosine, cosineT = cosineT, reducelr = reducelr, patience=patience_lr, early=early, patience_early=patience_early, learning_rate= lr, seed= seed, verbose= verbose, logfile=log_filename, isradam = isradam, weight_decay = weight_decay, amsgrad =amsgrad , best = best)\n",
    "\n",
    "\n",
    "   # need to save indim array and S, A values for be able to make the model autonomous\n",
    "    \n",
    "   tar = tarfile.open(MODEL_FILE, \"w:gz\");\n",
    "\n",
    "   tar.add(modelname);\n",
    "   tar.close();\n",
    "\n",
    "   try:\n",
    "      os.remove(modelname);\n",
    "      os.remove(log_filename);\n",
    "   except:\n",
    "      pass;\n",
    "\n",
    "\n",
    "   print(\"Relax!\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
