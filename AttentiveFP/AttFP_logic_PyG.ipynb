{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Datas import dataloader\n",
    "import pandas as pd\n",
    "target_list = ['Result0']\n",
    "batch_size = 64\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('esol.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loader,_,_,_ = dataloader(df, batch_size, target_list, shuffle=True, drop_last=False, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(atom_feature_dim=[0], batch=[877], edge_feat=[1810, 10], edge_index=[2, 1810], x=[877, 49], y0=[64])\n",
      "torch.Size([877, 49])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(data.x.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "trainloader = { \"train_loader\": train_loader }\n",
    "pickle.dump( trainloader, open( \"trainloader.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, BatchNorm1d, Dropout\n",
    "from torch.nn import Parameter as Param\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_add_pool, EdgePooling\n",
    "from torch_sparse import matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.typing import PairTensor, Adj, OptTensor, Size\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "class AttentionAtomEmbedding(MessagePassing):\n",
    "    \"\"\"\n",
    "    This function does only the atom embedding, not the molecule embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_in_channels: int, bond_in_channels: int,  fingerprint_dim: int, dropout: float, bias: bool = True, debug: bool = False,  **kwargs):\n",
    "        super(AttentionAtomEmbedding, self).__init__()\n",
    "\n",
    "        self.atom_in_channels = atom_in_channels\n",
    "        self.bond_in_channels = bond_in_channels\n",
    "        self.fingerprint_dim = fingerprint_dim\n",
    "        \n",
    "        # central atom feature only\n",
    "        self.atom_fc = Linear(atom_in_channels, fingerprint_dim, bias=bias)\n",
    "        # feature atom & bond\n",
    "        self.neighbor_fc = Linear(atom_in_channels + bond_in_channels, fingerprint_dim, bias=bias)\n",
    "        # align\n",
    "        self.align = Linear(2*fingerprint_dim,1, bias=bias)\n",
    "\n",
    "        self.attend = Linear(fingerprint_dim, fingerprint_dim, bias=bias)\n",
    "        self.debug = debug\n",
    "        self.rnn =  torch.nn.GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: Union[Tensor,PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \n",
    "        out = self.propagate(edge_index, x = x, edge_attr=edge_attr, size=size)\n",
    "        return out\n",
    "    \n",
    "    def message(self, x_i, x_j, edge_index: Adj, edge_attr: OptTensor, size) -> Tensor:\n",
    "\n",
    "        atom_feature = F.leaky_relu(self.atom_fc(x_i)) # line 36 # UpProjet => fp_dim \n",
    "        if self.debug:\n",
    "            print('a x_j:',x_j.shape,'x_i:',x_i.shape,'edge_attr:',edge_attr.shape)\n",
    "        \n",
    "        # neighbor_feature => neighbor_fc\n",
    "        neighbor_feature = torch.cat([x_j, edge_attr], dim=-1) # line 43  => Buv\n",
    "        if self.debug:\n",
    "            print('b neighbor_feature', neighbor_feature.shape)\n",
    "        \n",
    "        neighbor_feature = F.leaky_relu(self.neighbor_fc(neighbor_feature)) # line 44 => UpProject => fp_dim\n",
    "        if self.debug:\n",
    "            print('c neighbor_feature', neighbor_feature.shape)\n",
    "        \n",
    "        # feature_align\n",
    "        feature_align = torch.cat([atom_feature, neighbor_feature], dim=-1) # line 61\n",
    "        if self.debug:\n",
    "            print('d feature_align',feature_align.shape)\n",
    "        \n",
    "        # align score\n",
    "        align_score = F.leaky_relu(self.align(feature_align)) # line 63\n",
    "        if self.debug:\n",
    "            print('e align_score:', align_score.shape)\n",
    "        \n",
    "        # attention_weight using EdgePooling softmax method (molecules num_nodes)\n",
    "        attention_weight = EdgePooling.compute_edge_score_softmax(align_score, edge_index, edge_index.max().item() + 1)\n",
    "        if self.debug:\n",
    "            print('f attention_weight:', attention_weight.shape)\n",
    "        \n",
    "        neighbor_feature_transform = self.attend(self.dropout(neighbor_feature))\n",
    "        if self.debug:\n",
    "            print('g neighbor_feature_transform',neighbor_feature_transform.shape)\n",
    "    \n",
    "        #C_v = F.elu(scatter_add(context, edge_index, dim=0)) # line 74 scatter_add ?\n",
    "        context = torch.mul(attention_weight, neighbor_feature_transform)\n",
    "        if self.debug:\n",
    "            print('h context',context.shape)\n",
    "        context = F.elu(context) # line 74\n",
    "\n",
    "        # in orignal code they expend dimensions and use a mask before GRU!!!!\n",
    "        atom_feature = self.rnn(context, atom_feature) # line 77\n",
    "        \n",
    "        if self.debug:\n",
    "                print('i atom_feature end message end:',atom_feature.shape)\n",
    "        return atom_feature\n",
    "\n",
    "class AttentiveMolEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This function does the molecule embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, radius:int, T: int, fingerprint_dim: int, dropout: int, debug: bool = False):\n",
    "        super(AttentiveMolEmbedding, self).__init__()\n",
    "        # need to find the correct dimensions \n",
    "        self.mol_align = Linear(2*fingerprint_dim,1)\n",
    "        self.mol_expand = Linear(1,fingerprint_dim)\n",
    "\n",
    "        self.mol_attend = Linear(fingerprint_dim,fingerprint_dim)\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.debug = debug\n",
    "        \n",
    "        # let start with one AAE with 49 atom features and 10 bond features and FP of 200\n",
    "        self.atom_embedding =  AttentionAtomEmbedding(atom_in_channels = 49, bond_in_channels = 10,  fingerprint_dim = 200, dropout=0.3, debug = debug)\n",
    "\n",
    "        self.rnn = torch.nn.GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "        self.output = Linear(fingerprint_dim,1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        if self.debug:\n",
    "            print('0 Go Run!')\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_feat\n",
    "        \n",
    "        # Radius = 1 for the moment\n",
    "        # only one loop there: \n",
    "        activated_features =  self.atom_embedding(x, edge_index, edge_attr)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('1 back to h_v back from AE:',activated_features.shape) # message passing why from 4 => 3 atoms ?\n",
    "\n",
    "        # in orignal code they expend dimensions and use a mask before GRU!!!!\n",
    "        mol_feature = torch.sum(activated_features, dim=-1) # 113 \n",
    "        mol_feature_expanded = self.mol_expand(mol_feature.reshape(x.shape[0],1))\n",
    "        \n",
    "        if self.debug:\n",
    "            print('2 mol_feature',mol_feature_expanded.shape)\n",
    "        \n",
    "        mol_feature_expanded = F.relu(mol_feature_expanded) # 116\n",
    "        if self.debug:\n",
    "            print('3 activated_features', activated_features.shape)\n",
    "        \n",
    "        mol_cat_feature = torch.cat([mol_feature_expanded, activated_features], dim=-1)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('4 mol_cat_feature: ',mol_cat_feature.shape)\n",
    "       \n",
    "        mol_align = self.mol_align(mol_cat_feature)\n",
    "        \n",
    "        mol_align_score = F.softmax(F.leaky_relu(mol_align), dim=-1) # 127 ,129\n",
    "        \n",
    "        if self.debug:\n",
    "            print('5 mol_align_score:',mol_align_score.shape)\n",
    "        \n",
    "        activated_features_transform = self.mol_attend(self.dropout(activated_features)) # 132\n",
    "        if self.debug:\n",
    "            print('6 activated_features_transform:', activated_features_transform.shape)\n",
    "        \n",
    "        mol_context = torch.mul(mol_align_score, activated_features_transform) # 134\n",
    "        if self.debug:\n",
    "            print('7 mol_context' ,mol_context.shape)\n",
    "        mol_context = F.elu(mol_context) # line 136\n",
    "        \n",
    "        # in orignal code they expend dimensions and use a mask before GRU!!!!\n",
    "        mol_feature = self.rnn(mol_context, mol_feature_expanded) # 137\n",
    "        if self.debug:\n",
    "            print('8 mol_feature' ,mol_feature.shape)\n",
    "\n",
    "        activated_features_mol = F.relu(mol_feature)      #140     \n",
    "        out = self.output(self.dropout(mol_feature)) # 142\n",
    "        \n",
    "        out = global_add_pool(out, batch)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = pickle.load( open( \"trainloader.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = trainloader['train_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the model architecture\n",
    "# radius 1, T = 1, fpdim = 200\n",
    "model = AttentiveMolEmbedding(1, 1, 200, dropout = 0.2, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4055018424987793\n"
     ]
    }
   ],
   "source": [
    "# loop over data in a batch\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for data in train_loader:\n",
    "    #print(data)\n",
    "    #print(data.x.shape)\n",
    "    y = model(data)\n",
    "stop = time.time()\n",
    "print(stop-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
