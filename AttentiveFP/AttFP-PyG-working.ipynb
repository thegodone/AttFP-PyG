{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match same input of features\n",
    "from AttFPfeaturing import datagenerator, getdataloader\n",
    "#from Datas import dataloader\n",
    "import pandas as pd\n",
    "target_list = ['Result0']\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('esol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128/1128 [00:06<00:00, 175.21it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datagenerator(df, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = getdataloader(data, batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[803], edge_attr=[1612, 10], edge_index=[2, 1612], x=[803, 40], y=[64, 1])\n",
      "torch.Size([803, 40])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    print(data.x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.nn import Linear, BatchNorm1d, Dropout\n",
    "from torch.nn import Parameter as Param\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_add_pool, EdgePooling\n",
    "from torch_sparse import matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.typing import PairTensor, Adj, OptTensor, Size\n",
    "from torch_scatter import scatter_add\n",
    "import pickle\n",
    "\n",
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "class GatConvAtom(MessagePassing):\n",
    "    \"\"\"\n",
    "    This function does only the atom embedding, not the molecule embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_in_channels: int, bond_in_channels: int, fingerprint_dim: int, dropout: float, bias: bool = True, debug: bool = False, step = 0, **kwargs):\n",
    "        super(GatConvAtom, self).__init__()\n",
    "\n",
    "        self.atom_in_channels = atom_in_channels\n",
    "        self.bond_in_channels = bond_in_channels\n",
    "        self.fingerprint_dim = fingerprint_dim\n",
    "        self.step = step\n",
    "\n",
    "        if  self.step == 0 : \n",
    "            self.atom_fc = Linear(atom_in_channels, fingerprint_dim, bias=bias)\n",
    "            self.neighbor_fc = Linear(atom_in_channels + bond_in_channels, fingerprint_dim, bias=bias)\n",
    "        self.align = Linear(2*fingerprint_dim, 1, bias=bias)\n",
    "        self.attend = Linear(fingerprint_dim, fingerprint_dim, bias=bias)\n",
    "        self.debug = debug\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.rnn = torch.nn.GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, x: Union[Tensor,PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \n",
    "        out = self.propagate(edge_index, x = x, edge_attr=edge_attr, size=size)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index: Adj, edge_attr: OptTensor, size) -> Tensor:\n",
    "\n",
    "        if self.debug:\n",
    "            print('a x_j:',x_j.shape,'x_i:',x_i.shape,'edge_attr:',edge_attr.shape)\n",
    "        if  self.step == 0 :\n",
    "\n",
    "            x_i = F.leaky_relu(self.atom_fc(x_i)) # code 3 \n",
    "\n",
    "            # neighbor_feature => neighbor_fc\n",
    "            x_j = torch.cat([x_j, edge_attr], dim=-1) # code 8\n",
    "            if self.debug:\n",
    "                print('b neighbor_feature i = 0', x_j.shape)\n",
    "            \n",
    "            x_j = F.leaky_relu(self.neighbor_fc(x_j)) # code 9\n",
    "            if self.debug:\n",
    "                print('c neighbor_feature i = 0', x_j.shape)\n",
    "            \n",
    "        # align score\n",
    "        evu = F.leaky_relu(self.align(torch.cat([x_i, x_j], dim=-1))) # code 10\n",
    "        if self.debug:\n",
    "            print('d align_score:', evu.shape)\n",
    "        \n",
    "        avu = softmax(evu, edge_index[0], None, x_i.size(0))\n",
    "        \n",
    "        if self.debug:\n",
    "            print('e attention_weight:', avu.shape)\n",
    "\n",
    "        c_i = F.elu(torch.mul(avu, self.attend(self.dropout(x_i)))) # code 12\n",
    "\n",
    "        if self.debug:\n",
    "            print('f context',c_i.shape)\n",
    "            \n",
    "        x_i = self.rnn(c_i, x_i)\n",
    "        if self.debug:\n",
    "            print('g gru',c_i.shape)            \n",
    "\n",
    "        return x_i   \n",
    "\n",
    "class GatConvMol(MessagePassing):\n",
    "    \"\"\"\n",
    "    This function does the molecule embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, fingerprint_dim: int, dropout: int, debug: bool = False, step = 0):\n",
    "        super(GatConvMol, self).__init__()\n",
    "        # need to find the correct dimensions \n",
    "        self.step = step\n",
    "        self.mol_align = Linear(2*fingerprint_dim,1)\n",
    "        self.mol_attend = Linear(fingerprint_dim,fingerprint_dim)\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.debug = debug\n",
    "        self.rnn = torch.nn.GRUCell(fingerprint_dim, fingerprint_dim)\n",
    "\n",
    "    def forward(self, x: Union[Tensor,PairTensor], edge_index: Adj, size: Size = None) -> Tensor:\n",
    "        \n",
    "        out = self.propagate(edge_index, x = x, size=size)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index: Adj, size) -> Tensor:\n",
    "        if self.step == 0:\n",
    "            h_s =  torch.sum(x_i, dim=-1)\n",
    "            if self.debug:\n",
    "                print('pre-h_s:',h_s.shape,',x_i:', x_i.shape)            \n",
    "                \n",
    "            h_s =  h_s.unsqueeze(1).repeat(1, x_i.size(1)) # code 2\n",
    "            if self.debug:\n",
    "                print('1 mol_feature expanded',h_s.shape)\n",
    "\n",
    "        else:\n",
    "            h_s = x_i\n",
    "        \n",
    "        if self.debug:\n",
    "            print('2 activated_features', x_i.shape)\n",
    "             \n",
    "        esv = F.leaky_relu(self.mol_align(torch.cat([h_s, x_i], dim=-1))) # code 5\n",
    "        if self.debug:\n",
    "            print('3 mol_align_score:',esv.shape)\n",
    "        # this is a sotfmax per molecule  \n",
    "        asv = F.softmax(esv, dim=-1) # code 6\n",
    "    \n",
    "        if self.debug:\n",
    "            print('4 mol_align_score:',asv.shape)\n",
    "        \n",
    "        # this is not correct it should be more hs and not x_i there based on the paper supplementary table 3!\n",
    "        cs_i = F.elu(torch.mul(asv, self.mol_attend(self.dropout(h_s)))) # code 7 \n",
    "        if self.debug:\n",
    "            print('5 mol_context' ,cs_i.shape)\n",
    "            \n",
    "        x_i = self.rnn(cs_i, h_s) # code 8\n",
    "        \n",
    "        return x_i\n",
    "\n",
    "\n",
    "class AtomEmbedding(torch.nn.Module):\n",
    "    def __init__(self, atom_dim,  edge_dim, fp_dim, R=2, dropout = 0.2, debug=False):\n",
    "        super(AtomEmbedding, self).__init__()\n",
    "        self.R = R\n",
    "        self.debug = debug\n",
    "        self.conv = torch.nn.ModuleList([GatConvAtom(atom_in_channels=atom_dim, bond_in_channels= edge_dim, fingerprint_dim=fp_dim, dropout = dropout, debug=debug, step = i) for i in range(self.R)])  # GraphMultiHeadAttention\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for i in range(self.R):\n",
    "            if self.debug:\n",
    "                print(x.shape)\n",
    "            \n",
    "            x = self.conv[i](x, edge_index, edge_attr) # code 1-12\n",
    "            if self.debug:\n",
    "                print(x.shape)    \n",
    "        return x\n",
    "    \n",
    "\n",
    "class MoleculeEmbedding(torch.nn.Module):\n",
    "    def __init__(self, fp_dim, dropout, debug, T=2):\n",
    "        super(MoleculeEmbedding, self).__init__()\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.conv =torch.nn.ModuleList([GatConvMol(fp_dim, dropout, debug, step = i) for i in range(self.T)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.T):\n",
    "            x = self.conv[i](x, edge_index) # code 1-7\n",
    "        return x\n",
    "\n",
    "class AttentiveFPdebug(torch.nn.Module):\n",
    "    def __init__(self, atom_in_dim, edge_in_dim, fingerprint_dim=200, R=2, T=2, dropout=0.2,  debug = True, outdim=1):\n",
    "        super(AttentiveFPdebug, self).__init__()\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.dropout = dropout\n",
    "        # call the atom embedding Phase\n",
    "        self.convsAtom = AtomEmbedding(atom_in_dim, edge_in_dim, fingerprint_dim, R, debug) \n",
    "        self.convsMol = MoleculeEmbedding(fingerprint_dim, dropout, debug, T )\n",
    "\n",
    "        # fast down project could be much more sofisticated! (ie  Feed Forward Network with multiple layers )\n",
    "        self.out = Linear(fingerprint_dim, outdim) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        print(self.training)        \n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        x = F.dropout(self.convsAtom(x, edge_index, edge_attr), p=self.dropout, training=self.training) # atom Embedding       \n",
    "        #x = F.dropout(self.convsMol(x, edge_index), p=self.dropout, training=self.training) # molecule Embedding\n",
    "        x = self.out(global_add_pool(x, batch))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveFP(torch.nn.Module):\n",
    "    def __init__(self, atom_in_dim, edge_in_dim, fingerprint_dim=32, R=2, T=2, dropout=0.2,  debug = False, outdim=1):\n",
    "        super(AttentiveFP, self).__init__()\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.debug = debug\n",
    "        self.dropout = dropout\n",
    "        # call the atom embedding Phase\n",
    "        self.convsAtom = AtomEmbedding(atom_in_dim, edge_in_dim, fingerprint_dim, R, debug)\n",
    "        # call the Mol embedding Phase\n",
    "        self.convsMol = MoleculeEmbedding(fingerprint_dim, dropout, debug, T )\n",
    "\n",
    "        # fast down project could be much more sofisticated! (ie  Feed Forward Network with multiple layers )\n",
    "        self.out = Linear(fingerprint_dim, outdim) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        print(self.training)\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_feat\n",
    "        x = F.dropout(self.convsAtom(x, edge_index, edge_attr), p=self.dropout, training=self.training) # atom Embedding\n",
    "        #x = F.dropout(self.convsMol(x, edge_index), p=self.dropout, training=self.training) # mol Embedding\n",
    "        x = self.out(F.dropout(x, p=self.dropout, training=self.training)) # final prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the model architecture\n",
    "model = AttentiveFPdebug(40, 10, 200, R= 2, T=2, dropout = 0.2, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentiveFPdebug(\n",
       "  (convsAtom): AtomEmbedding(\n",
       "    (conv): ModuleList(\n",
       "      (0): GatConvAtom(\n",
       "        (atom_fc): Linear(in_features=40, out_features=200, bias=True)\n",
       "        (neighbor_fc): Linear(in_features=50, out_features=200, bias=True)\n",
       "        (align): Linear(in_features=400, out_features=1, bias=True)\n",
       "        (attend): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=True, inplace=False)\n",
       "        (rnn): GRUCell(200, 200)\n",
       "      )\n",
       "      (1): GatConvAtom(\n",
       "        (align): Linear(in_features=400, out_features=1, bias=True)\n",
       "        (attend): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=True, inplace=False)\n",
       "        (rnn): GRUCell(200, 200)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (convsMol): MoleculeEmbedding(\n",
       "    (conv): ModuleList(\n",
       "      (0): GatConvMol(\n",
       "        (mol_align): Linear(in_features=400, out_features=1, bias=True)\n",
       "        (mol_attend): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (rnn): GRUCell(200, 200)\n",
       "      )\n",
       "      (1): GatConvMol(\n",
       "        (mol_align): Linear(in_features=400, out_features=1, bias=True)\n",
       "        (mol_attend): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (rnn): GRUCell(200, 200)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[1750], edge_attr=[3616, 10], edge_index=[2, 3616], x=[1750, 40], y=[128, 1])\n",
      "True\n",
      "0.10774350166320801\n"
     ]
    }
   ],
   "source": [
    "# loop over data in a batch\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y_true = []\n",
    "y_out = []\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    y = model(data)\n",
    "    y_out.extend(y.squeeze().detach().cpu().numpy())\n",
    "    y_true.extend(data.y.squeeze().detach().cpu().numpy())\n",
    "    break\n",
    "    \n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print                                             \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "################ optimizer #####################\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Time\n",
    "class Model:\n",
    "    def __init__(self, R , \n",
    "                 T, \n",
    "                 atom_input_dim = 49,\n",
    "                 bond_input_dim = 10,\n",
    "                 dropout = 0.0,\n",
    "                 fpdim= 200,\n",
    "                 output_dim = 1, \n",
    "                 device=0,\n",
    "                 cuda=True,\n",
    "                 debug = False):\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.atom_input_dim = atom_input_dim\n",
    "        self.bond_input_dim = bond_input_dim\n",
    "        self.dropout = dropout\n",
    "        self.fpdim = fpdim\n",
    "        self.debug = debug\n",
    "        self.output_dim = output_dim\n",
    "        self.outputreal=self.output_dim \n",
    "\n",
    "        # device\n",
    "        if cuda:\n",
    "            self.device = torch.device('cuda:%i' %device)\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        self.model = AttentiveFPdebug(self.atom_input_dim,\n",
    "                            self.bond_input_dim,\n",
    "                            self.fpdim,\n",
    "                            self.R,\n",
    "                            self.T, \n",
    "                            self.dropout, \n",
    "                            self.debug)\n",
    "        \n",
    "    def fit(self, data_train,\n",
    "            data_valid,\n",
    "            epochs,\n",
    "            loss,\n",
    "            path = '',\n",
    "            learning_rate = 5e-3,\n",
    "            reducelr = True,\n",
    "            reducefactor = 0.8,\n",
    "            early = False,\n",
    "            patience_early = 40,\n",
    "            patience = 60,\n",
    "            cosine = True,\n",
    "            cosineT = 20,\n",
    "            seed=None,\n",
    "            verbose=True,\n",
    "            logfile=None,\n",
    "            isradam = False,\n",
    "            weight_decay = 0,\n",
    "            amsgrad = False,\n",
    "            best = True):\n",
    "\n",
    "        # check that data is a dataloader\n",
    "        self.path = path\n",
    "        self.nboutputfull = self.outputreal\n",
    "        \n",
    "        if seed and seed >=0:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            \n",
    "        # right away initialize the fit by saving the model\n",
    "        self.save_model(self.path, weights=True)\n",
    "\n",
    "        if isradam:\n",
    "            print('Use RAdam')\n",
    "            opt = RAdam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else: \n",
    "            print('Use Adam')\n",
    "            opt = torch.optim.Adam(self.model.parameters(), lr=learning_rate, amsgrad=amsgrad, weight_decay=weight_decay)\n",
    "\n",
    "            \n",
    "        # reduce on plateau\n",
    "        if reducelr:\n",
    "            plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=patience, factor=reducefactor, verbose=False)\n",
    "                    \n",
    "        # cosine annealing\n",
    "        if cosine:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt,T_0=cosineT, T_mult=2) \n",
    "\n",
    "        # adding early stopping\n",
    "        if early:\n",
    "            early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "            \n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # default parameters if we save the best model\n",
    "        best_loss_val = 3e+6\n",
    "\n",
    "        for t in range(0,epochs):\n",
    "            if logfile is not None:\n",
    "                mode = 'a+'\n",
    "                with open(logfile, mode) as f:\n",
    "                    f.write(\"Epoch {0} / {1}\".format(t+1, epochs))\n",
    "            if verbose:\n",
    "                print(\"Epoch {0} / {1}\".format(t+1, epochs))\n",
    "            epoch_loss = 0.0\n",
    "            count_train = 0\n",
    "            \n",
    "            y_true = torch.tensor([], device=self.device)\n",
    "            y_out = torch.tensor([], device=self.device)\n",
    "            for batch_i, batch_data in enumerate(data_train):\n",
    "                self.model.train()\n",
    "                batch_data = batch_data.to(self.device)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "               \n",
    "                y_batch_pred = self.model(batch_data)\n",
    "                \n",
    "                y_true = torch.cat((y_true,),0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_loss = 0\n",
    "\n",
    "                # still need to handle correctly multitarget\n",
    "                y_out = torch.cat((y_out, y_batch_pred),0)\n",
    "\n",
    "                y_true_tensor = torch.tensor([], device=self.device)\n",
    "                for k in range(self.outputreal): # nboutputfull\n",
    "                    y_true_tensor = torch.cat(( y_true_tensor,batch_data.y.unsqueeze(-1)),1)\n",
    "                # only count std and avg predictions not the rest of the outputs \n",
    "                for k in range(self.outputreal):\n",
    "                    batch_loss += loss(y_batch_pred[:,k].view(-1),batch_data.y)\n",
    "                y_true = torch.cat(( y_true,y_true_tensor),0)\n",
    "\n",
    "                # classical method backward and one step optimizer\n",
    "                batch_loss.backward()\n",
    "                # avoid \n",
    "                \n",
    "                opt.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                loss_train = loss(y_out,y_true).detach().cpu()\n",
    "\n",
    "            val_loss_all = self.predict(data_valid, loss, verbose=0) \n",
    "            val_loss = np.sum(val_loss_all) # change mean to sum for val loss like for train loss\n",
    "                    \n",
    "            if val_loss < best_loss_val: # the mean is taken to consider the multi target case as well\n",
    "                best_loss_val = val_loss\n",
    "                                    \n",
    "            if verbose:\n",
    "                print(\"Train:\",loss_train,\" ,Valid:\",val_loss_all,\" ,Best Loss Val:\",best_loss_val)\n",
    "\n",
    "            if logfile is not None:\n",
    "                with open(logfile, 'a+') as f:\n",
    "                    f.write('\\tTrain: %f\\tVals:' %loss_train)\n",
    "                    for vla in val_loss_all:\n",
    "                        f.write(str(vla))\n",
    "                        f.write('\\t')\n",
    "                    f.write('\\n')\n",
    "                \n",
    "            # compute the early stopping \n",
    "            if early:\n",
    "                early_stopping(val_loss, self.model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "                \n",
    "            # reduce on Plateau\n",
    "            if reducelr:\n",
    "                plateau.step(val_loss)\n",
    "        \n",
    "        self.save_model(self.path, weights=True)\n",
    "\n",
    "    def predict(self, data, loss=None, verbose=1):\n",
    "        self.model.eval()\n",
    "        epoch_loss = {}\n",
    "\n",
    "        for k in range(self.outputreal):\n",
    "            epoch_loss[k] = 0\n",
    "                \n",
    "        count_train = 0\n",
    "\n",
    "        output_augm_vec = []\n",
    "        ytrue_augm_vec = []\n",
    "        augm_idx_vec = []\n",
    "\n",
    "        # initialize the loss vecs\n",
    "        pred_loss_vec = []\n",
    "        true_loss_vec = []\n",
    "        \n",
    "        for batch_data in data:\n",
    "            batch_data = batch_data.to(self.device)\n",
    "            y_batch_pred = self.model(batch_data)\n",
    "            \n",
    "            #print(y_batch_pred.shape)\n",
    "            #print(batch_data.y.shape)\n",
    "        \n",
    "            batch_loss = {}\n",
    "            count_train += batch_data.y.size(0)\n",
    "            for k in range(self.outputreal):\n",
    "                # scaled data\n",
    "                if k==0:\n",
    "                    batch_loss[k] = loss(y_batch_pred[:,k].view(-1), batch_data.y)\n",
    "                if k>0:\n",
    "                    batch_loss[k] = loss(y_batch_pred[:,k].view(-1),  batch_data.y)\n",
    "                epoch_loss[k] += batch_data.num_graphs * batch_loss[k].item()\n",
    "        \n",
    "        return [epoch_loss[k]/count_train for k in range(self.outputreal)]\n",
    "    \n",
    "    def apply_model(self, data):\n",
    "        self.model.eval()\n",
    "\n",
    "        # initialize the loss vecs\n",
    "        y_out = torch.tensor([], device=self.device)\n",
    "        y_true = torch.tensor([], device=self.device)\n",
    "\n",
    "        for batch_data in data:\n",
    "            y_true_tensor = torch.tensor([], device=self.device)\n",
    "            batch_data = batch_data.to(self.device)\n",
    "            y_batch_pred = self.model(batch_data)\n",
    "            \n",
    "            # still need to handle correctly multitarget\n",
    "            y_out = torch.cat((y_out, y_batch_pred),0)\n",
    "            for k in range(self.outputreal):\n",
    "                y_true_tensor = torch.cat(( y_true_tensor,(batch_data.y).unsqueeze(-1)),1)\n",
    "            y_true = torch.cat(( y_true,y_true_tensor),0)\n",
    "        return y_out , y_true  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_model(self, path, weights=False):\n",
    "        \"\"\" \n",
    "        method to save the trained model\n",
    "        :param path: the full path where to save the model, str\n",
    "        :param weights: boolean, whether to consider only the weights, otherwise the full model (archi+weights)\n",
    "        \"\"\"\n",
    "        if weights:\n",
    "            torch.save(self.model.cpu().state_dict(), path)\n",
    "            self.model = self.model.to(self.device)\n",
    "        else:\n",
    "            torch.save(self.model.cpu(), path)\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "            \n",
    "    def load_model(self, path, weights=False):\n",
    "        \"\"\" \n",
    "        method to save the trained model\n",
    "        :param path: the full path from where to load the model, str\n",
    "        :param weights: boolean, whether to consider only the weights, otherwise the full model (archi+weights)\n",
    "        \"\"\"\n",
    "        if weights:\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "            self.model = self.model.to(self.device)\n",
    "        else:\n",
    "            self.model.load(path)\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [00:06<00:00, 160.36it/s]\n",
      "100%|██████████| 113/113 [00:00<00:00, 156.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentiveFPdebug(\n",
      "  (convsAtom): AtomEmbedding(\n",
      "    (conv): ModuleList(\n",
      "      (0): GatConvAtom(\n",
      "        (atom_fc): Linear(in_features=40, out_features=200, bias=True)\n",
      "        (neighbor_fc): Linear(in_features=50, out_features=200, bias=True)\n",
      "        (align): Linear(in_features=400, out_features=1, bias=True)\n",
      "        (attend): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=False, inplace=False)\n",
      "        (rnn): GRUCell(200, 200)\n",
      "      )\n",
      "      (1): GatConvAtom(\n",
      "        (align): Linear(in_features=400, out_features=1, bias=True)\n",
      "        (attend): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=False, inplace=False)\n",
      "        (rnn): GRUCell(200, 200)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (convsMol): MoleculeEmbedding(\n",
      "    (conv): ModuleList(\n",
      "      (0): GatConvMol(\n",
      "        (mol_align): Linear(in_features=400, out_features=1, bias=True)\n",
      "        (mol_attend): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "        (rnn): GRUCell(200, 200)\n",
      "      )\n",
      "      (1): GatConvMol(\n",
      "        (mol_align): Linear(in_features=400, out_features=1, bias=True)\n",
      "        (mol_attend): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "        (rnn): GRUCell(200, 200)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "Use Adam\n",
      "Epoch 1 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(81.7093)  ,Valid: [18.28957748413086]  ,Best Loss Val: 18.28957748413086\n",
      "Validation loss decreased (inf --> 18.289577).  Saving model ...\n",
      "Epoch 2 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(11.7163)  ,Valid: [7.369380950927734]  ,Best Loss Val: 7.369380950927734\n",
      "Validation loss decreased (18.289577 --> 7.369381).  Saving model ...\n",
      "Epoch 3 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(8.4593)  ,Valid: [7.426486492156982]  ,Best Loss Val: 7.369380950927734\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 4 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(7.0343)  ,Valid: [6.111359596252441]  ,Best Loss Val: 6.111359596252441\n",
      "Validation loss decreased (7.369381 --> 6.111360).  Saving model ...\n",
      "Epoch 5 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(6.4292)  ,Valid: [5.472192764282227]  ,Best Loss Val: 5.472192764282227\n",
      "Validation loss decreased (6.111360 --> 5.472193).  Saving model ...\n",
      "Epoch 6 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(6.2065)  ,Valid: [5.276220321655273]  ,Best Loss Val: 5.276220321655273\n",
      "Validation loss decreased (5.472193 --> 5.276220).  Saving model ...\n",
      "Epoch 7 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(6.0025)  ,Valid: [5.125145435333252]  ,Best Loss Val: 5.125145435333252\n",
      "Validation loss decreased (5.276220 --> 5.125145).  Saving model ...\n",
      "Epoch 8 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.9305)  ,Valid: [5.061330795288086]  ,Best Loss Val: 5.061330795288086\n",
      "Validation loss decreased (5.125145 --> 5.061331).  Saving model ...\n",
      "Epoch 9 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.7945)  ,Valid: [4.874934673309326]  ,Best Loss Val: 4.874934673309326\n",
      "Validation loss decreased (5.061331 --> 4.874935).  Saving model ...\n",
      "Epoch 10 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.6866)  ,Valid: [4.828923225402832]  ,Best Loss Val: 4.828923225402832\n",
      "Validation loss decreased (4.874935 --> 4.828923).  Saving model ...\n",
      "Epoch 11 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.6571)  ,Valid: [4.930422782897949]  ,Best Loss Val: 4.828923225402832\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 12 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.6098)  ,Valid: [4.720582962036133]  ,Best Loss Val: 4.720582962036133\n",
      "Validation loss decreased (4.828923 --> 4.720583).  Saving model ...\n",
      "Epoch 13 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.5141)  ,Valid: [4.655423641204834]  ,Best Loss Val: 4.655423641204834\n",
      "Validation loss decreased (4.720583 --> 4.655424).  Saving model ...\n",
      "Epoch 14 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.4611)  ,Valid: [4.593287467956543]  ,Best Loss Val: 4.593287467956543\n",
      "Validation loss decreased (4.655424 --> 4.593287).  Saving model ...\n",
      "Epoch 15 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.4460)  ,Valid: [4.590764045715332]  ,Best Loss Val: 4.590764045715332\n",
      "Validation loss decreased (4.593287 --> 4.590764).  Saving model ...\n",
      "Epoch 16 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.3102)  ,Valid: [4.550363540649414]  ,Best Loss Val: 4.550363540649414\n",
      "Validation loss decreased (4.590764 --> 4.550364).  Saving model ...\n",
      "Epoch 17 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.2869)  ,Valid: [4.532248497009277]  ,Best Loss Val: 4.532248497009277\n",
      "Validation loss decreased (4.550364 --> 4.532248).  Saving model ...\n",
      "Epoch 18 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.2269)  ,Valid: [4.640576362609863]  ,Best Loss Val: 4.532248497009277\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 19 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.1927)  ,Valid: [4.497194290161133]  ,Best Loss Val: 4.497194290161133\n",
      "Validation loss decreased (4.532248 --> 4.497194).  Saving model ...\n",
      "Epoch 20 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "Train: tensor(5.3188)  ,Valid: [5.129852294921875]  ,Best Loss Val: 4.497194290161133\n",
      "EarlyStopping counter: 1 out of 40\n",
      "Epoch 21 / 200\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c342edc2e4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosineT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosineT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducelr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreducelr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience_early\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misradam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misradam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mamsgrad\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-60285d51cca0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data_train, data_valid, epochs, loss, path, learning_rate, reducelr, reducefactor, early, patience_early, patience, cosine, cosineT, seed, verbose, logfile, isradam, weight_decay, amsgrad, best)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# classical method backward and one step optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "################## basic parameters ############\n",
    "target_list = ['target']\n",
    "output_dim = 1\n",
    "patience = 40\n",
    "verbose = 1\n",
    "\n",
    "################ OCHEM integration part ###############\n",
    "import configparser\n",
    "import tarfile\n",
    "import pickle\n",
    "config = configparser.ConfigParser();\n",
    "config.read('config-attfp-my.cfg');\n",
    "\n",
    "\n",
    "def getConfig(section, attribute, default=\"\", type=\"int\"):\n",
    "    try:\n",
    "        if type == 'int':\n",
    "            return config.getint(section,attribute,fallback = default);\n",
    "        if type == 'float':\n",
    "            return config.getfloat(section,attribute,fallback = default);\n",
    "        if type == 'bool':\n",
    "            return config.getboolean(section,attribute,fallback = default);\n",
    "        else:\n",
    "            return config.get(section,attribute,fallback = default);\n",
    "\n",
    "    except:\n",
    "        return default;\n",
    "\n",
    "TRAIN = getConfig(\"Task\",\"train_mode\",True,'bool');\n",
    "MODEL_FILE = getConfig(\"Task\",\"model_file\",\"\",\"\");\n",
    "TRAIN_FILE = getConfig(\"Task\",\"train_data_file\" ,\"esol.csv\",\"\");\n",
    "APPLY_FILE = getConfig(\"Task\",\"apply_data_file\", \"train.csv\",\"\");\n",
    "RESULT_FILE = getConfig(\"Task\",\"result_file\", \"results.csv\",\"\");\n",
    "\n",
    "best = getConfig(\"Details\",\"best\",True,'bool');\n",
    "isradam = getConfig(\"Details\",\"Adam\",False,'bool');\n",
    "nbepochs = getConfig(\"Details\", \"nbepochs\", 10,'int');\n",
    "R = getConfig(\"Details\", \"R\", 2,'int');\n",
    "T = getConfig(\"Details\", \"T\", 2,'int');\n",
    "fpdim = getConfig(\"Details\", \"fpdim\", 200,'int');\n",
    "lr = getConfig(\"Details\", \"lr\", 0.001, 'float');\n",
    "batch_size = getConfig(\"Details\", \"batch\", 32, 'int');\n",
    "dropout = getConfig(\"Details\", \"dropout\", 0.2, 'float');\n",
    "seed = getConfig(\"Details\", \"seed\", 420, 'int');\n",
    "cosine =  getConfig(\"Details\", \"cosine\", True,'bool')\n",
    "cosineT = getConfig(\"Details\", \"cosineT\", 14, 'int');\n",
    "reducelr =  getConfig(\"Details\", \"reducelr\",  False,'bool');\n",
    "patience_lr = getConfig(\"Details\", \"patiencelr\", 40,'int');\n",
    "patience_early = getConfig(\"Details\", \"patience_early\", 40, 'int');\n",
    "early = getConfig(\"Details\", \"early\", True, 'bool') \n",
    "gpu = getConfig(\"Details\", \"gpu\", 0, 'int');\n",
    "weight_decay = getConfig(\"Details\", \"weight_decay\", 0.0, 'float');\n",
    "amsgrad = getConfig(\"Details\", \"amsgrad\", False, 'bool')     \n",
    "    \n",
    "log_filename = 'model.log';\n",
    "modelname = \"model.pt\";\n",
    "\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "\n",
    "criterion =  torch.nn.MSELoss()\n",
    "\n",
    "### need to change to bool now!\n",
    "if TRAIN:\n",
    "\n",
    "   data = [];\n",
    "   first = True;\n",
    "   for line in open(TRAIN_FILE, \"r\").readlines():\n",
    "      if not first:\n",
    "         arr = line.strip().split(\",\");\n",
    "         data.append([arr[0], float(arr[1]) ]);\n",
    "      first = False;\n",
    "\n",
    "   total = len(data);\n",
    "   ntrain = int(0.9 * total);\n",
    "\n",
    "   df_train = pd.DataFrame(data[:ntrain], columns = [\"smiles\", \"target\"] );\n",
    "   df_valid = pd.DataFrame(data[ntrain:], columns = [\"smiles\", \"target\"] );\n",
    "    \n",
    "    \n",
    "   traindata = datagenerator(df_train, target_list) \n",
    "\n",
    "   valdata = datagenerator(df_valid, target_list) \n",
    "    \n",
    "   \n",
    "   train_loader= getdataloader(traindata, batch_size,  shuffle=True,  drop_last=False)\n",
    "    \n",
    "    \n",
    "   valid_loader = getdataloader(valdata, batch_size,  shuffle=False,  drop_last=False)\n",
    "\n",
    "   # need to add classifier ouptut option\n",
    "   model = Model(R=2, T =2, atom_input_dim = 40, bond_input_dim = 10,  dropout=0.3, fpdim=200,  output_dim=1)\n",
    "    \n",
    "   print(model.model)\n",
    "    \n",
    "   model.fit(train_loader, valid_loader, nbepochs, criterion, path = modelname, cosine=cosine, cosineT = cosineT, reducelr = reducelr, patience=patience_lr, early=early, patience_early=patience_early, learning_rate= lr, seed= seed, verbose= verbose, logfile=log_filename, isradam = isradam, weight_decay = weight_decay, amsgrad =amsgrad , best = best)\n",
    "\n",
    "\n",
    "   # need to save indim array and S, A values for be able to make the model autonomous\n",
    "    \n",
    "   tar = tarfile.open(MODEL_FILE, \"w:gz\");\n",
    "\n",
    "   tar.add(modelname);\n",
    "   tar.close();\n",
    "\n",
    "   try:\n",
    "      os.remove(modelname);\n",
    "      os.remove(log_filename);\n",
    "   except:\n",
    "      pass;\n",
    "\n",
    "\n",
    "   print(\"Relax!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
